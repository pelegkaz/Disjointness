\section{Lower Bound}
\paragraph{Introduction}
Usually in lower bound using information theory techniques, we firstly move to the problem of disjointness where $n=1$ denoted as $\text{AND}_k$ which is the problem where every player has one bit and they need to answer whether they all got 1 or not. After moving to this problem, we calculate how much information is needed in order to solve it.
\subsection{$AND_k$ Information Cost}
\paragraph{Introduction}
We are going to bound the information cost for a protocol that solves $AND_k$. Our analysis is divided into three blocks: \newline
1 - Using the error of the protocol in order to conclude it must know some information about specific player's input. \newline
2 - Distinguishing this input is leaking some information (specifically KL-Divergence) \newline
3 - Concluding this for the general information cost of the protocol \newline
\paragraph{Definitions}
\begin{definition}
    Denote two sets of transcripts: \newline
    $T_1$ - transcripts which are ended with positive answers (there is an intersection). \newline
    $T_0$ - transcripts which are ended with negative answers (there is no intersection).
\end{definition}

\paragraph{Protocol properties}
For a transcript $\pi$ and player $i$ there is a function $q_i(\pi, x_i) \in [0,1]$ where
\begin{equation*}
    Pr[\pi | x] = \prod_{i=1}^{k}q_i(\pi, x_i )
\end{equation*}

\begin{definition}
\begin{equation*}
    \lambda _i (\pi) = \frac{q_i(\pi, 0)}{q_i(\pi, 1)}
\end{equation*}
We should think about this as how much this transcript prefers that $x_i = 0$ over $x_i = 1$.
\end{definition}

\begin{definition}
For $\alpha \in \mathbb{R}$ where $\alpha \geq 1$, let us define a set of transcripts 
\begin{equation*}
    A = \{\pi | \forall_{i \in [k]} \lambda_i (\pi) < \alpha \}
\end{equation*}
\end{definition}


\paragraph{Part 1 - Protocol Error Analysis}
\begin{lemma}
For any input $x \in \{0,1\}^k$, and any transcript $\pi$. For $Z(x) = \{i \in k | x_i = 0\}$, it is true that  \newline
\begin{equation*}
    \frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \prod_{i \in Z(x)} \lambda_i(\pi)
\end{equation*}
\end{lemma}
\begin{proof}
By definition of $q$ function
\begin{equation*}
    \frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \frac{\prod_{i \in [k]} q_i (\pi, x_i)}{\prod_{i \in [k]} q_i (\pi, 1)}
\end{equation*}
By definition of $Z(x)$
\begin{equation*}
    \frac{\prod_{i \in [k]} q_i (\pi, x_i)}{\prod_{i \in [k]} q_i (\pi, 1)} = \prod_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)}
\end{equation*}
By definition of $\lambda_i (\pi)$
\begin{equation*}
    \prod_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)} = \prod_{i \in Z(x)} \lambda_i (\pi)
\end{equation*}
\end{proof}

\begin{lemma}
    \begin{equation*}
        \Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k)
    \end{equation*}
    The set in which the transcripts do not prefer 0 over 1 is not very common under $x = 0^k$.
\end{lemma}
\begin{proof}
For $T_0, T_1$ defined as mentioned, let us bound the probability for $A \bigcap T_0$ and $A \bigcap T_1$. \newline

1. For $A \bigcap T_0$: \newline
Let there be $\pi \in A \bigcap T_0$. \newline
For this $\pi$, let us use the previous lemma for $x = 0^k$
\begin{equation*}
    \frac{Pr[\pi | X = 0^k]}{Pr[\pi | X = 1^k]} = \prod_{i \in Z(0^k)} \lambda_i(\pi)
\end{equation*}
Since $Z(0^k) = [k]$
\begin{equation*}
    \frac{Pr[\pi | X = 0^k]}{Pr[\pi | X = 1^k]} = \prod_{i \in [k]} \lambda_i(\pi)
\end{equation*}
Since $\pi \in A$, we know that $\forall_{i \in [k]}\lambda_i(\pi) < \alpha$
\begin{equation*}
    \frac{Pr[\pi | X = 0^k]}{Pr[\pi | X = 1^k]} < \alpha^{k}
\end{equation*}
Summing over all $\pi \in A \bigcap T_0 $
\begin{equation*}
    \Pr[A \bigcap T_0 | x = 0^k] < \alpha ^k \Pr[A \bigcap T_0 | x = 1^k]
\end{equation*}
Since $A \bigcap T_0 \subseteq T_0$
\begin{equation*}
    \alpha ^k \Pr[A \bigcap T_0 | x = 1^k] \leq \alpha ^k \Pr[T_0 | x = 1^k]
\end{equation*}
Since the answer of disjointness is 1 for the input $x=1^k$, we know that $\{T_0 | x=1^k\} = \{\text{ERROR} | x=1^k\}$ .
\begin{equation*}
    \alpha ^k \Pr[T_0 | x = 1^k] = \alpha ^k \Pr[\text{ERROR} | x = 1^k]
\end{equation*}
For a protocol which errors $\epsilon$ for any input
\begin{equation*}
    \Pr[\text{ERROR} | x = 1^k] \leq \epsilon
\end{equation*}
We now have that 
\begin{equation*}
    \Pr[A \bigcap T_0 | x = 0^k] \leq \alpha^k \epsilon
\end{equation*}
2. For $A \bigcap T_1$: \newline
Since $A \bigcap T_1 \subseteq T_1$
\begin{equation*}
    \Pr[A \bigcap T_1 | x = 0^k] \leq \Pr[T_1 | x = 0^k]
\end{equation*}
Since the answer of disjointness is 0 for the input $x=0^k$, we know that $\{T_1 | x=0^k\} = \{\text{ERROR} | x=0^k\}$ .
\begin{equation*}
    \Pr[T_1 | x = 0^k] = \Pr[\text{ERROR} | x = 0^k]
\end{equation*}
For a protocol which errors $\epsilon$ for any input
\begin{equation*}
    \Pr[\text{ERROR} | x = 0^k] \leq \epsilon
\end{equation*}
We now have that 
\begin{equation*}
    \Pr[A \bigcap T_1 | x = 0^k] \leq \epsilon
\end{equation*}

3. To conclude: \newline
\begin{equation*}
    \Pr[A| x = 0^k] = \leq \Pr[A \bigcap T_0 | x = 0^k] + \Pr[A \bigcap T_1 | x = 0^k] < \epsilon (1 + \alpha ^k)
\end{equation*}
\end{proof}

\begin{lemma}
    For $A_{i}^\complement = \{\pi | \lambda_i(\pi) \geq \alpha\}$, there exists $i$ where
    \begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
    \end{equation*}
\end{lemma}
\begin{proof}
Using the previous lemma,
\begin{equation*}
    \Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k)
\end{equation*}
Using set compliment
\begin{equation*}
    \Pr[A^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k)
\end{equation*}
Since $A^\complement = \{\pi | \exists_i \lambda_i(\pi) \geq \alpha\}$, we know that 
\begin{equation*}
    A^\complement = \bigcup_{i} A_{i}^\complement
\end{equation*}
Therefore 
\begin{equation*}
     \sum_{i=1}^{k} \Pr[A_i^\complement| x = 0^k] \geq \Pr[A^\complement| x = 0^k]
\end{equation*}
Combining these facts
\begin{equation*}
    \sum_{i=1}^{k} \Pr[A_i^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k)
\end{equation*}
Therefore there exists $i$ where 
\begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
    \end{equation*}
\end{proof}

That is the finish line of this part. We got a set of transcripts that has a nice probability under $x = 0^k$ where the transcripts prefer strongly 0 over 1 for some index. We may use this technique in different ways depends on our distribution of inputs. For small $k$ our distribution gives a high probability for $0^k$ which is pretty convenient. For other distribution, we want to use the other method. \newline
This is a rough method in order to use the fact that the protocol has to be "biased" in terms of $\lambda_i$ in order to have a low error. \newline
\paragraph{Part 2 - Divergence}
In this part, we are going to show that the set we found in the last part is contributing large enough divergence. This will be enough since $x = 0^k$ has a high probability under our distribution. \newline
Let us analyze the connection between $\lambda_i(\pi)$ to its divergence. \newline
\begin{lemma}
    For 0-1 distribution $\mu$, player $i$ and transcript $\pi$
    \begin{equation*}
        \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] = \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}
    \end{equation*}
    \begin{equation*}
        \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi] = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
    \end{equation*}
\end{lemma}
\begin{proof}
    By Bayes  theorem
    \begin{equation*}
        \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] = \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi|X_{-i} = 0^{k-1}]} = 
    \end{equation*}
    By the law of total probability for $\pi|X_{-i} = 0^{k-1}$ with $X_i=0,1$
    \begin{equation*}
        \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi,X_i=0|X_{-i} = 0^{k-1}] + Pr[\pi,X_i=1|X_{-i} = 0^{k-1}]} = 
    \end{equation*}
    Using conditional probability for the denominator
    \begin{equation*}
        \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}] + \Pr[\pi | X=0^{i-1}10^{k-i}]\Pr[X_i=1|X_{-i}=0^{k-1}]} =
    \end{equation*}
    Since the input is drawn using product distribution
    \begin{equation*}
        \frac{\Pr[\pi | X=0^k]\mu(0)}{\Pr[\pi | X=0^k]\mu(0) + \Pr[\pi | X=0^{i-1}10^{k-i}]\mu(1)} = 
    \end{equation*}
    Divide both with $\Pr[\pi | X=0^k]$
    \begin{equation*}
        \frac{\mu(0)}{\mu(0) + \frac{\Pr[\pi | X=0^{i-1}10^{k-i}]}{\Pr[\pi | X=0^k]}\mu(1)} = 
    \end{equation*}
    Using the definition of $\lambda_i(\pi)$
    \begin{equation*}
        \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}
    \end{equation*}
    This proves the first equation.
    For the second equation
    \begin{equation*}
        \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi] = 1 - \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]
    \end{equation*}
    Using the first equation
    \begin{equation*}
        1 - \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] = 1 - \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}
    \end{equation*}
    Some algebraic expansion
    \begin{equation*}
        1 - \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\frac{\mu(1)}{\lambda_i(\pi)}}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
    \end{equation*}
\end{proof}
\begin{lemma}
    For a specific transcript $\pi$ and player $i$ where $\lambda_i(\pi) \geq \alpha$, for big enough $n$
    \begin{equation*}
        D_{KL}\left(\Dfrac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i} = 0^{k-1}} \right) \geq \frac{\mu(1)}{4}
    \end{equation*}
\end{lemma}
\begin{proof}
By definition of KL-Divergence
\begin{equation*}
    D_{KL}\left(\Dfrac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i} = 0^{k-1}} \right) = 
\end{equation*}
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 0 | X_{-i}=0^{k-1}]}\right) +
\end{equation*}
\begin{equation*}
    \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 1 | X_{-i}=0^{k-1}]}\right)
\end{equation*}
\paragraph{Part 1 of the divergence:}
Since the input is drawn using product distribution
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 0 | X_{-i}=0^{k-1}]}\right) = \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]}{\mu(0)}\right) = 
\end{equation*}
Using the previous lemma
\begin{equation*}
    \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{\frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}}{\mu(0)}\right) = 
\end{equation*}
Some algebraic expansion
\begin{equation*}
    \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\right) \geq
\end{equation*}
Using $\lambda_i(\pi) \geq \alpha$
\begin{equation*}
    \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\alpha}}\right) =
\end{equation*}
Since $\mu(0) = 1 - \mu(1)$
\begin{equation*}
    \frac{\mu(0)}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\right) = 
\end{equation*}
Some algebraic expansion
\begin{equation*}
    \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(\frac{1}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) =
\end{equation*}
Some more algebraic expansion
\begin{equation*}
    \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(1 + \frac{\mu(1)(1 -\frac{1}{\alpha})}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) \geq
\end{equation*}
Using $\log(1+x) \sim x$ for $x \rightarrow 0$
\begin{equation*}
    \frac{\mu(0)(\mu(1)(1 -\frac{1}{\alpha}))}{2\left(1 - \mu(1)(1 -\frac{1}{\alpha})\right)^2} \geq \frac{\mu(1)}{2}
\end{equation*}

\paragraph{Part 2 of the divergence:}
Since the input is drawn using product distribution
\begin{equation*}
    \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 1 | X_{-i}=0^{k-1}]}\right) = \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]}{\mu(1)}\right) = 
\end{equation*}
Using the previous lemma
\begin{equation*}
    \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}\log\left(\frac{\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}}{\mu(1)}\right) = 
\end{equation*}
Some algebraic expansion
\begin{equation*}
    \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}\log\left(\frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) = -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)}
\end{equation*}
Pay attention that since $ \mu(1) \geq 0$, $\mu(0) \geq \frac{1}{2}$ and $\lambda_i(\pi) \geq \alpha$
\begin{equation*}
    \lambda_i(\pi)\mu(0) + \mu(1) \geq \lambda_i(\pi)\mu(0) \geq \frac{\lambda_i(\pi)}{2} \geq \frac{\alpha}{2}
\end{equation*}
Since $\frac{log(x)}{x} \rightarrow 0$ when $x \rightarrow \infty$, for big enough $\alpha$
\begin{equation*}
    -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)} \geq -\frac{\mu(1)}{4}
\end{equation*}
\paragraph{For conclusion}
If $\lambda_i(\pi) \geq \alpha$, for big enough $n$:
\begin{equation*}
     D_{KL}\left(\Dfrac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i} = 0^{k-1}} \right) \geq \frac{\mu(1)}{2} -\frac{\mu(1)}{4} = \frac{\mu(1)}{4}
\end{equation*}
\end{proof}

\paragraph{Part 3 - Information Summary}
\begin{theorem}
\begin{equation*}
    I(X;\Pi) \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\end{theorem}
\begin{proof}
By definition of $X$
\begin{equation*}
    I(X;\Pi) = I(X_i,X_{-i};\Pi)
\end{equation*}
Using chain rule for mutual information
\begin{equation*}
    I(X_i,X_{-i};\Pi) = I(X_{-i}; \Pi) + I(X_i;\Pi|X_{-i})
\end{equation*}
Since mutual information is nonnegative
\begin{equation*}
    \overbrace{I(X_{-i}; \Pi)}^{\geq 0} + I(X_i;\Pi|X_{-i}) \geq I(X_i;\Pi|X_{-i})
\end{equation*}
Using the mutual information - kl-divergence relation
\begin{equation*}
    I(X_i;\Pi|X_{-i}) = \mathbb{E}_{x, \pi \sim \mu} \left( D_{KL}\left(\Dfrac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Picking only a specific element ($x=0^k$) in the expectation sum
\begin{equation*}
    \mathbb{E}_{x, \pi \sim \mu} \left( D_{KL}\left(\Dfrac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Looking only at transcripts in $A_{i}^\complement$
\begin{equation*}
    \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Using Part 1
\begin{equation*}
    \mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Using Part 2
\begin{equation*}
    \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4}
\end{equation*}
Since $\mu(1) = n^{-\frac{1}{k}}$ and $\mu(0) = 1 - n^{-\frac{1}{k}}$
\begin{equation*}
    \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4} = \left (1 - n^{-\frac{1}{k}} \right)^{k} \cdot \frac{1 - \epsilon (1 + \alpha ^k)}{k} \cdot \frac{n^{-\frac{1}{k}}}{4} \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\end{proof}