\documentclass[11pt]{article}

% Encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}

% Page layout
\usepackage{geometry}
\geometry{margin=1in}

% Math and environments
\usepackage{amsmath,amssymb,amsthm,mathtools}

% Lists and colors
\usepackage{xcolor}
\usepackage{enumitem}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}

% Project macros and theorem environments
\input{macros}

\title{Protocol Compression for Similar Known Distribution}
\author{Peleg Kazaz}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
 
\end{abstract}

\section{Introduction}

\begin{definition}
Notation and transcript distributions for a two-party public-coin protocol:
\begin{itemize}[leftmargin=*, itemsep=2pt]
  \item $x$ and $y$ denote Alice's and Bob's inputs, respectively. A complete transcript is denoted by $\ell$.
  \item Let $\Pi$ be the transcript random variable. Define
  \[
    \pi_{xy}(\ell) \coloneqq \Pr[\Pi=\ell \mid X=x, Y=y].
  \]
  For alternating protocols, this factors as
  \[
    \pi_{xy}(\ell) = p_x(\ell)\, p_y(\ell),
  \]
  where $p_x(\ell)$ aggregates the probabilities of Alice's messages in $\ell$ conditioned on $X=x$ (and the public randomness), and $p_y(\ell)$ aggregates the probabilities of Bob's messages in $\ell$ conditioned on $Y=y$ (Alice estimation on Bob's messages).
  \item The $x$-marginal transcript distribution is
  \[
    \pi_x(\ell) \coloneqq p_x(\ell)\, q_x(\ell),
  \]
  where $q_x(\ell)$ is the probability of Bob's messages in $\ell$ under the conditional distribution $Y\mid X=x$. Similarly,
  \[
    \pi_y(\ell) \coloneqq p_y(\ell)\, q_y(\ell),
  \]
  where $q_y(\ell)$ is the probability of Alice's messages in $\ell$ under $X\mid Y=y$ (Bob's estimation on Alice's messages given only his input).
\end{itemize}

Similar-known-distribution setting: the players do not know the true input distribution, but they know some reference transcript distributions. We write these as $\tilde{\pi}_x$ and $\tilde{\pi}_y$, which share the same local factors $p_x$ and $p_y$:
\[
  \tilde{\pi}_x(\ell) = p_x(\ell)\, \tilde{q}_x(\ell)
  \quad\text{and}\quad
  \tilde{\pi}_y(\ell) = p_y(\ell)\, \tilde{q}_y(\ell).
\]

We measure the difference between the known distribution and the original distribution using Kullback--Leibler divergence $D(\cdot\,\|\,\cdot)$ and define
\[
  D_x \coloneqq D\big(\tilde{\pi}_x \,\|\, \pi_x\big),\qquad
  D_y \coloneqq D\big(\tilde{\pi}_y \,\|\, \pi_y\big),\qquad
  D \coloneqq D_x + D_y.
\]
\end{definition}

\begin{theorem}
Let $\pi$ be a public-coin two-party protocol on inputs $(X,Y) \sim \mu$ and let $I \coloneqq IC_{\mu}(\pi)$. Assume the players have access to a reference distribution $\tilde{\mu}$ that induces transcript marginals $\tilde{\pi}_x, \tilde{\pi}_y$ with
\[
  D_x \coloneqq D(\tilde{\pi}_x \,\|\, \pi_x),\qquad
  D_y \coloneqq D(\tilde{\pi}_y \,\|\, \pi_y),\qquad
  D \coloneqq D_x + D_y.
\]
Then for every $\varepsilon > 0$, there exists a protocol $\tau$ that $\varepsilon$-simulates $\pi$ under $\mu$ and has communication
\[
  \norm{\tau} \le 2^{\,O\left(\frac{I + D}{\varepsilon}\right)}.
\]
\end{theorem}

\begin{proof}
We start by relating the distance between Alice and Bob's estimations on each other's messages to the information complexity of $\pi$ and the divergence $D$ between the known (reference) distribution and the true distribution. The key is that if both the information cost $I$ and the total divergence $D$ are small, then the players estimation errors are also small. \\

Directly from definitions:
\[
I = I(\Pi; X \mid Y) + I(\Pi; Y \mid X) = \mathbb{E}_{(x,y)\sim\mu} \left[ D\big( \pi_{xy} \,\|\, \pi_y \big) + D\big( \pi_{xy} \,\|\, \pi_x \big) \right] = \mathbb{E}_{x,y,\ell \sim \pi_{x,y}} \left[ \log \frac{\pi_{xy}(\ell)}{\pi_y(\ell)} + \log \frac{\pi_{xy}(\ell)}{\pi_x(\ell)} \right].
\]

Integrating the known distribution into the estimation error:
\begin{align*}
    I
    &= \mathbb{E}_{x,y,\ell \sim \pi_{x,y}} 
       \left[
          \log \frac{\pi_{xy}(\ell)}{\tilde{\pi}_y(\ell)}
          + \log \frac{\tilde{\pi}_y(\ell)}{\pi_y(\ell)}
          + \log \frac{\pi_{xy}(\ell)}{\tilde{\pi}_x(\ell)}
          + \log \frac{\tilde{\pi}_x(\ell)}{\pi_x(\ell)}
       \right] \\
    &= \mathbb{E}_{x,y,\ell \sim \pi_{x,y}} 
       \left[
          \log \frac{\pi_{xy}(\ell)}{\tilde{\pi}_y(\ell)}
          + \log \frac{\pi_{xy}(\ell)}{\tilde{\pi}_x(\ell)}
       \right]
          - D_y
          - D_x \\
    &= \mathbb{E}_{x,y,\ell \sim \pi_{x,y}} 
       \left[
          \log \frac{p_x(\ell)}{\tilde{q}_y(\ell)}
          + \log \frac{p_y(\ell)}{\tilde{q}_x(\ell)}
       \right]
       - D \,.
\end{align*}

Finally,
\begin{align*}
    I + D 
    &= \mathbb{E}_{x,y,\ell \sim \pi_{x,y}} 
    \left[
       \log \frac{p_x(\ell)}{\tilde{q}_y(\ell)}
       + \log \frac{p_y(\ell)}{\tilde{q}_x(\ell)}
    \right] \,.
\end{align*}

\begin{lemma}
Define the set of transcripts $B_{\varepsilon} \coloneqq \{ \, \ell : p_x(\ell) > 2^{(I+D+1)/\varepsilon} \, \tilde{q}_y(\ell) \text{ or } p_y(\ell) > 2^{(I+D+1)/\varepsilon} \, \tilde{q}_x(\ell) \}$. Then $\pi_{x,y}(B_{\varepsilon}) < \varepsilon$.
\end{lemma}

\begin{proof}
   Using Markov's inequality and the previous equation,
   \[
     \Pr\!\left[ \log \frac{p_x(\ell)}{\tilde{q}_y(\ell)}
     + \log \frac{p_y(\ell)}{\tilde{q}_x(\ell)} > \frac{I + D + 1}{\varepsilon} \right]
     \le \frac{\varepsilon \, (I + D)}{I + D + 1}
     < \varepsilon.
   \]
   Since $B_{\varepsilon} \subseteq \left\{ \ell : \log \frac{p_x(\ell)}{\tilde{q}_y(\ell)} + \log \frac{p_y(\ell)}{\tilde{q}_x(\ell)} > \tfrac{I + D + 1}{\varepsilon} \right\}$, the claim follows.
\end{proof}

\begin{remark}[Interpretation of the lemma]
For most transcripts, if a message is unlikely to be sent by Alice, then Bob's estimate for this message is also small. Equivalently, except on an $\varepsilon$-fraction of transcripts, the players' estimations of each other's messages are close to the true behavior.
\end{remark}
\end{proof}

\paragraph{Discussion.}
What is $D$? In this work we measure the discrepancy via
$D \coloneqq D_x + D_y = D(\tilde{\pi}_x\,\|\,\pi_x) + D(\tilde{\pi}_y\,\|\,\pi_y)$,
which compares the reference transcript marginals to the true ones. A seemingly natural alternative, $D(\mu\,\|\,\tilde{\mu})$, is generally unsuitable.

\begin{example}[Why $D(\mu\,\|\,\tilde{\mu})$ is inadequate]
Consider distributions $\mu$ and $\tilde{\mu}$ over inputs with $X=Y$  (under both). 
In this case, the modification of the original distribution is not disturbing the player's ability to estimate each other's messages.
When observing the divergences, we pay attention that.
The conditional distributions $Y\mid X$ and $X\mid Y$ are constant in both the original and the modified distributions, so the induced factors agree and $D = D_x + D_y = 0$.
This fact matches the intuition.
However, the divergence $D(\mu\,\|\,\tilde{\mu})$ can be arbitrarily large (even infinite). Thus $D(\mu\,\|\,\tilde{\mu})$ does not capture the relevant notion of discrepancy for compression.
\end{example}

\nocite{*}
\bibliographystyle{alpha}
\bibliography{refs}

\end{document}

