\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{times}
\usepackage{paralist}
\usepackage[usenames]{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{authblk}
\usepackage[normalem]{ulem}
\usepackage[ruled,boxed,vlined]{algorithm2e}
\usepackage{float}
\usepackage[colorlinks,urlcolor=black,citecolor=black,linkcolor=black]{hyperref}

\title{Multiparty Disjointness on Product Distributions}
\author{Peleg Kazaz}
\date{June 2020}


%General math style
\newcommand{\fnstyle}[1]{\mathsf{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\coloneq}{:=}
\newcommand{\st}{\medspace \middle| \medspace}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\prob}[1]{\ensuremath{\text{\textsc{#1}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator{\poly}{poly}

%Comments, TODO, etc.
\newcommand{\hide}[1]{ }
\newcommand{\note}[1]{ { \color{blue} #1 } }
\newcommand{\Rnote}[1]{ { \color{magenta} #1 } }
\newcommand{\TODO}[1]{ {\color{red} #1 }}

%Probability
\newcommand{\Ber}{\mathsf{B}}
\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\newcommand{\given}{\medspace \middle| \medspace}
\newcommand{\rv}[1]{\mathbf{#1}}

%Communication & information
\newcommand{\CC}{\mathrm{CC}}
\DeclareMathOperator*{\MI}{I}
\DeclareMathOperator*{\CIC}{CIC}
\DeclareMathOperator{\HH}{H} 


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{invariant}{Invariant}
\newtheorem*{remark}{Remark}

\renewcommand{\include}{\input}

\begin{document}

\maketitle


\section{Introduction}
The world of communication complexity discusses the settings in which there are many nodes (sometimes called players). Each node has its own input. For a given function, the goal is to design a protocol in which the nodes calculate the function over the whole input (e.g all of the nodes' inputs) together using minimum communication. \newline
In this world, one of the most important problems (sometimes called "mother of problems") is the Set Disjointness problem. In this problem, every node gets a n-bit array and they are asked whether there is an index $i$ where the $i$'th bit is turned on in all of their inputs. \newline
There are many different settings in which this protocol can be made. The communication between players can be broadcast (shared blackboard model) or unicast (the coordinator). Moreover, the protocol may use randomness or not. The inputs of the players may be drawn using a distribution and in this case we can ask what is the expected error over this distribution (instead of worst case for each input). \newline
In this paper, we are going to discuss the case in which the inputs are drawn from a product distribution. In this case the inputs are independent and we are asked to design a protocol which doesn't error typically. 
\subsection{Notations}
\Rnote{Not touching this for now}
We use the popular notation: We are going to have $k$ players. For $i \in [k], X_i \in \{0,1\}^{n}$ - the $i$'th player's input. Sometimes we are going to think of $X_i$ as a subset of $[n]$ where $X_i = \{j \in [n] | X_{ij} = 1\}$. In this notation, the distjointness problem is to decide whether $\cap^{k}_{j=1}X_j = \emptyset$. \newline
\section{Holding The Universe}
Let us describe a point of view for the set distjoiness problem: we are going to hold a universe $U \subseteq [n]$. The universe is the set of indexes we do care about them e.g the indexes where there might be a intersection over the inputs (maintaining $\bigcap_i x_i \subseteq U$). Set Distjointness problem asks whether $\bigcap_i x_i = \emptyset$ but knowing $\bigcap_i x_i \subseteq U$, says we can easily consider only the question whether $(\bigcap_i x_i) \cap U = \emptyset$. The protocol is going to act in rounds where in each round it \emph{reveals indexes} which don't hold an intersection and get rid of them (thus reducing the universe's size). Such indexes are the indexes which some player has 0 on them. Therefore the protocol is going to focus about revealing zeros in the players' input efficiently. \newline
\subsection{Revealing Zeros} 
Let us first examine a protocol which helps a single player to reveal many zeros using small communication in a specific scenario. Later on we describe how this scenario is created.
\begin{lemma}
    For a settings where there is a set $A \subseteq [N] $, $k$ players (which know $A$) and player $i$ holds a set $A_i$. There are public numbers $m, n$ and $m \leq |A| \leq n$. If $|A_i| \leq \frac{|A|}{m}, A_i \subseteq A$. There is a deterministic protocol that reveals a set $B \subseteq A \setminus A_i$ and $|B| \geq \frac{m}{2}$ and costs $klog(n)$ bits of communication (using the coordinator settings). 
\end{lemma}
\begin{proof}
    Let us divide $A$ to groups sized $\frac{m}{2}$ where the last group may be sized up to $m -1$: $B_1, B_2, ...$. This is a division that every player knows even before communicating since they all have $A, m$. \newline
    Pay attention that there are more than $|A_i|$ groups since $  |A_i|(\frac{m}{2}) < |A_i|(m) \leq |A|$. Due to the pigeonhole principle, there is a group $B_j$ which is disjoint to $A_i$. \newline
    Since the division is public, player $i$ simply sends $j$ to the other players. Every player understands $B_j$ and know that $A_i \cap B_j = \emptyset$. 
    The only communication used is $j \in [\frac{|A|}{m/2}]$, therefore $klog(n)$. 
\end{proof}
\section{Sequential Point Of View}
In order to find a player with some zeros, we are going to think about set disjointness a little differently. 
We are going to consider a sequential point of view. Let us imagine a process in which we start with the set of all points ([n]). We consider the inputs of the players one after another and intersect their inputs with the result. After intersecting with the last player's set, we end up with the intersection of all sets. Therefore our question is whether this set is empty or not. Moreover, if the last set is empty, one of the players must have subtracted a large amount of elements in this process. Finding this player helps us in order to reveal large amount of zeros in this player's input.

Define sets $[N] = A_0 \supseteq A_1 \supseteq \ldots \supseteq A_k$, as follows:
for each $i = 1,\ldots,k$,
\begin{equation*}
  A_i \coloneq \cap_{j = 1}^i x_j,
\end{equation*}
so that for each $i \in [k]$ we have $A_i = A_{i-1} \cap x_i$.
Our goal in the $\prob{Disj}$ problem is to determine whether or not $A_k = \emptyset$. \newline

We can do the same thing in a specific universe $U$.
\begin{equation*}
  A_i(U) \coloneq \cap_{j = 1}^i x_j \cap U
\end{equation*}

\paragraph{Critical players.}
We claim that if $\bigcap_i x_i = \emptyset$, then there is at least one player $i$ such that $A_i$ is significantly smaller than $A_{i - 1}$;
in other words, $x_i$ eliminates a significant fraction of elements from consideration.
\begin{lemma}
    Let there be a universe $U \subseteq [N]$ ($U \neq \emptyset$). Denote $n = |U|$. \newline
  If $\bigcap_i x_i = \emptyset$,
  then there is some player $i \in [k]$
  with
  \begin{enumerate}
    \item 
      $|A_{i}(U)| / {|A_{i-1}(U)|} < 1 / n^{1/k}$
    \item $|A_{i-1}(U)| \geq n^{1-\frac{i-1}{k}}$
  \end{enumerate}
  \label{lemma:narrow}
\end{lemma}
\begin{proof}
  We first prove that there is an index $j$ which satisfies the first property, and then we show
  that the minimal such $j$ also has the second property.

  Let $j$ be the maximal index such that 
  \begin{equation*}
    A_{j-1}(U) \neq \emptyset.
  \end{equation*}
  There is such a $j$, because $A_0(U) = U \neq \emptyset$.

  We claim that $A_{j}(U) = \emptyset$.
  If $j \neq k$, then this must be true, because $j$ is the maximal index such that $A_{j-1}(U) \neq \emptyset$;
  and if $j = k$, then $A_j(U) = A_k(U) = U \bigcap_i x_i = \emptyset$ by assumption.
  Thus,
  \begin{equation*}
    \frac{|A_j(U)|}{|A_{j-1}(U)|} = \frac{0}{|A_{j-1}(U)|} = 0 < \frac{1}{n^{1/k}}
  \end{equation*}
  So we know that (1) holds for $j$.
  Let us denote $j^\star$ the minimal index which satisfies (1).
  Because of its minimality, for all $i \in [j^\star-1]$ we have
  \begin{equation*}
    \frac{|A_i(U)|}{|A_{i-1}(U)|} \geq \frac{1}{n^{1/k}}.
  \end{equation*}
  Then we get that
  \begin{align*}
    \frac{|A_{j^\star - 1}(U)|}{|A_0(U)|} = 
    \frac{|A_{j^\star - 1}(U)|}{|A_{j^\star-2}(U)|} \cdot \frac{|A_{j^\star-2}(U)|}{|A_{j^\star-3}(U)|} \cdot \ldots \cdot \frac{|A_1(U)|}{|A_0(U)|}
    \geq
    \left(  \frac{1}{n^{1/k}} \right)^{j^\star - 1}
    =
    \frac{1}{n^{\frac{j^\star - 1}{k}}}.
  \end{align*}
  Since $|A_0(U)| = n$, 
  \begin{align*}
      |A_{j^\star - 1}(U)| \geq n^{1 - \frac{j^\star-1}{k}}
  \end{align*}
    Therefore $A_{j^\star}(U) \neq \emptyset$.
\end{proof}

For given inputs $x_1, x_2, ... x_n$, we say that player $i$ is \emph{critical} in $U$ if $i$ satisfies the conditions of the lemma. \newline
We can now denote $DISJ_i(U) = DISJ \cap \{\text{i is critical in U}\}$ and also $\forall_{U \subseteq [N]} DISJ = \bigcup_i DISJ_i(U)$
\paragraph{Significant Players}
Note that a player does not know whether it is critical or not, as this depends 
on the other players' inputs;
but each player can compute the \emph{probability} that it is critical, given its input.
For a player $i$ with input $x_i$, universe $U$ define
\begin{equation*}
  \gamma_i(x_i, U) \coloneq \Pr_{(\rv{X}_{-i}) \sim \mu}\left[ DISJ_i (U) \given \rv{X}_i = x_i \right].
\end{equation*}
We say that player $i$ is \emph{significant} if $\gamma_i \geq \eps / kN^{1-\frac{1}{k}}$. \newline

\section{$O(kN^{1-\frac{1}{k}})$-Bit Protocol}

\subsection{Overview of the Protocol}
Throughout the protocol, we maintain a \emph{universe}, $U \subseteq [N]$,
with the property that $\bigcap_i x_i \subseteq U$.
We initially have $U = [N]$.
The protocol operates in \emph{iterations}, where each iteration has two possible outcomes:
\begin{enumerate}[I.]
  \item We eliminate at least a $1/\Theta(|U|^{1/k})$-fraction of elements from $U$, or
  \item We halt and guess that $\bigcap_i x_i \neq \emptyset$. This guess is correct with high probability over the inputs.
\end{enumerate}
After at most $O(N^{1 - 1/k})$ iterations, if we have not yet halted,
then $|U| < c$ for some constant. At this point every player just sends his input to the coordinator which computes the disjointness.

\subsection{The Protocol}
For a given product distribution $\mu : \left(\set{0,1}^N\right)^k \rightarrow [0,1]$, we describe a protocol that errs with probability $\epsilon$.
Our protocol uses public randomness and has $\tilde{O}(kN^{1-\frac{1}{k}})$ bits of expected communication.

Initially, $U = \set{1,\ldots,N}$, and $n = N$.
\begin{enumerate}[(1)]
  \item Termination Condition: If $|U| \leq N^{1 - 1/k}$, each player $i$ sends $x_i$ to the coordinator, who computes and outputs the answer. 
  \item Otherwise, the coordinator asks each player $i$ whether $i$ is significant in $U$ (i.e., whether $\gamma_i(x_i, U) \geq \eps / kN^{1-\frac{1}{k}}$).
  \item If no player is significant, the coordinator outputs ``not disjoint''. Otherwise, let $i$ be the
    first significant player. The coordinator informs every player that player $i$ has been selected.
  \item The coordinator and the players use the public randomness to sample \TODO{how many needed?}
    sets $A_{i-1}^{(1)},\ldots$ from the distribution of $\rv{A}_{i-1} \cap U$.
  \item Player $i$ finds the index $j$ of the first set $A_{i-1}^{(j)}$ such that
    \begin{equation*}
      |A_{i-1}^{(j)} \cap X_i| \leq \frac{|A_{i-1}^{(j)}|}{n^{1/k}}.
    \end{equation*}
    It sends the index $j$ to the coordinator, who forwards it to the other players.
  \item The participants use the protocol from lemma 2.1 in order to discover $B \subseteq A_{i-1}^{(j)} \setminus X_i $ where $ |B| > \frac{n^{1/k}}{2}$ 
  \item All participants set:
    \begin{itemize}
      \item $U \leftarrow U \setminus B$,
      \item $n \leftarrow |U|$,
      \item $x_i \leftarrow x_i \cap U$,
      \item $\mu \leftarrow \mu $ (isn't updated)
    \end{itemize}
\end{enumerate}

%\begin{algorithm}[H]
  %\SetAlgoLined
  %initialization\;
  %\While{not at end of this document}
  %{
    %read current\;
    %\eIf{understand}{go to next section\;current section becomes this one\;}
    %{go back to the beginning of current section\;}
  %}
  %\caption{How to write algorithms}
%\end{algorithm}

%In each iteration $r$, the coordinator asks each of the $k$ players whether or not they are significant.
%If no player is significant, the coordinator halts and outputs ``not disjoint''.
%If there is some significant player, the coordinator chooses the first player $i$ that is significant,
%and sends the index $i$ to all players.
%
%Next, using public randomness, the coordinator and the players sample infinitely many \TODO{(figure out how many really)}
%iid inputs $(X_1, \ldots , X_k)^1, \ldots (X_1, \ldots , X_k)^2,\ldots \sim \mu((\rv{X_1}, \ldots , \rv{X_k}))$.
%Player $i$ finds the first index $j$ such that he is critical in $(X_1, \ldots X_{i-1}, x_i, X_{i+1}, \ldots , X_k)^j$,
%sends $j$ to the coordinator, and then
%
%and uses 
%We use this sample in order to find elements in $[n] \setminus x_i$ as following: Critical property claims that $|A_{i-1}^j \cap x_i| / |A_{i-1}^j| < 1/n_{r-1}^{1/k}$. Now we split $A_{i-1}^j$ to sets of size $\frac{n_{r-1}^{1/k}}{2}$ by order denoted $Z^{r}_1, \ldots , Z^{r}_m$. .By counting argument, one of them doesn't intersect with $A_{i-1}^j \cap x_i$ - let us denote its index by $l$. Player $i$ sends the index $j$ to the coordinator, along with the index $l$ for the right $Z^{r}_l$ to the coordinator.
%The coordinator disseminates $j, l$ to all the other players.
%
%Observe that $Z^{r}_l \subseteq \overline{X_i}$,
%so the elements of $Z^{r}_l$ can now be removed from the universe:
%\begin{equation*}
  %U \leftarrow U_{r-1} \setminus (Z^{r}_l).
%\end{equation*}

%If $n_r \leq n^{1-\frac{1}{k}}$, every player sends his input to the coordinator which calculate the disjointness and sends the output to every player.

%\paragraph{Clarifications}
%We strongly use the fact that this is a product distribution where everyone can sample the inputs and parse them and the critical player's input doesn't affect the distribution. \newline
%For specific inputs, one of the player must be critical if there is an intersection (as proved in the lemma) but it is not easy to know which one of them is critical. 
%\TODO{up to here}



\subsection{Properties of Our Protocol}
Our protocol is using random coins and is described in the coordinator model. It uses an expected value of $O(kn^{1-\frac{1}{k}})$ bits of communication between the players.
\subsection{Analysis}
\paragraph{Error Analysis}
Let us analyze the error for a specific round. The overall error will be the sum of these errors. \newline
For a specific round $r$ (with a universe $U_r$), we error only if we guessed not disjointness incorrectly. For this to happen, the input should be disjoint and any player isn't significant in $U_r$. (Although this is not enough since the protocol may stop before round r) \newline

$\mu(\text{Err in round r}) \leq \mu(DISJ \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

As proved in lemma 3.1 $DISJ = \bigcup_j DISJ_j(U_r)$ \newline

$ \mu(DISJ \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) \leq \Sigma_j \mu(DISJ_j(U_r) \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

Now just relaxing the event:\newline
$ \Sigma_j \mu(DISJ_j(U_r) \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) \leq \Sigma_j \mu(DISJ_j(U_r) \land \gamma_j(X_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

Let us handle a specific element in this sum. We can take the Expectation over $X_j$ \newline

$\mu(DISJ_j(U_r) \land \gamma_j(X_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) = \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(DISJ_j(U_r) \land \gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}} | X_j = x_j)]$ \newline

This make $\gamma_j$ a constant. \newline

$\underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(DISJ_j(U_r) \land \gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}} | X_j = x_j)] = \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(DISJ_j(U_r) | X_j = x_j) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}]$ \newline

Recall the definition $\gamma_i(x_i, U) \coloneq \Pr_{(\rv{X}_{-i}) \sim \mu}\left[ DISJ_i (U) \given \rv{X}_i = x_i \right]$ \newline

$\underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(DISJ_j(U_r) | X_j = x_j) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}] = \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\gamma_j(x_j, U_r) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}]$ \newline

Now we got an expectation over a r.v with a domain $[0, 1]$ multiplied by the indicator of the event it is in $[0, \frac{\epsilon}{kN^{1-\frac{1}{k}}}]$. \newline

$\underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\gamma_j(x_j, U_r) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}] \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}$ \newline

Summing over all critical players: \newline
$\Sigma_j \mu(DISJ_j(U_r) \land \gamma_j(X_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) \leq k\frac{\epsilon}{kN^{1-\frac{1}{k}}} = \frac{\epsilon}{N^{1-\frac{1}{k}}}$
\newline

To conclude the error in a specific round:
$\mu(\text{Err in round r}) \leq \frac{\epsilon}{N^{1-\frac{1}{k}}}$.\newline

Now we are ready to sum the overall error:

$\mu(Err) = \Sigma_{r} \mu(\text{Err in round r}) \leq \#\{\text{number of rounds}\} \frac{\epsilon}{N^{1-\frac{1}{k}}}$. \newline
In our protocol there are $O(N^{1-\frac{1}{k}})$ so
$\mu(Err) \in O(\epsilon)$

\paragraph{Communication Analysis}
Let us first analyze the expected communication for a single round: \newline

Every player sends a bit whether he is significant or not - $k$ bits.\newline

The coordinator informs everyone who is the chosen significant - $klogk$ bits. \newline

The index $j$ of $A_{i-1}^{(j)}$ is sent. \newline
This is a random variable which has a geometric distribution since $A_{i-1}^{(j)}$ are independent. \newline
$\Pr[|A_{i-1}^{(j)} \cap X_i| \leq \frac{|A_{i-1}^{(j)}|}{n^{1/k}} | X_i = x_i] \leq \Pr[DISJ_i(U) $

$ \mathop{\mathbb{E}} [J] = \frac{1}{\Pr[\text{$i$ is critical in $U_{r-1}$} \land \text{DISJ} | X_i = x_i]} \leq \frac{k}{\epsilon}$ \newline
By Jensen's inequality, $\mathop{\mathbb{E}} [\log(J)] \leq \log(\mathop{\mathbb{E}} [J]) \leq \log(\frac{k}{\epsilon}) = \log(k) + \log(\frac{1}{\epsilon})$.  \newline
So in total the cost of the round is $k + k\log(k) + k(\log(k) + \log(\frac{1}{\epsilon})) + k\log(n) \in O(k(\log(n) + \log(\frac{1}{\epsilon}))$ and we eliminated $(n_r^{\frac{1}{k}}/2)$ zeros. \newline
As can be seen in the appendix - there are $O(n^{1-\frac{1}{k}})$ rounds.
For the last round we pay at most $kn^{1-\frac{1}{k}} \log(n)$. \newline
So the total communication cost is  $O(kn^{1-\frac{1}{k}}(\log(\frac{1}{\epsilon}) + \log(n)))$.
\section{k-n relation analysis}
\paragraph{$k < \log(n)$:}
For this case $k$ is pretty small. meaning $kn^{1/k} \approx n^{1/k}$. So we are pretty tight (our lower bound is almost like our upper one).
\paragraph{$k = \alpha\log(n)$:}
\begin{equation*}
    n^{1/k} = n^{\frac{1}{\alpha\log(n)}} = 2^{\frac{\log(n)}{\alpha\log(n)}} = 2^{1/\alpha}
\end{equation*}
so
\begin{equation*}
    n^{1-\frac{1}{k}} > n/2
\end{equation*}
Moreover, in this scenario:
\begin{equation*}
    p = \frac{1}{n^{1/k}} = \frac{1}{2^{\frac{1}{\alpha}}} 
\end{equation*}
\begin{equation*}
    h(p) = c 
\end{equation*}
\begin{equation*}
    h(\rv{X}) = cnk
\end{equation*}
So our lower bound may also be fine.
\paragraph{$k \in \omega(\log(n))$:}
This is an interesting case. Most of the elements in our input should be 1. 
Let us denote:
\begin{equation*}
    p_{ij} = \Pr[i \in X_j]
\end{equation*}
\begin{equation*}
    \Pr[\neg \text{DISJ}] \leq \Sigma_{i=1}^{n}\Pr[i \in \cap_{j=1}^{k}X_j] = \Sigma_{i=1}^{n}\Pi_{j=1}^{k}p_{ij}
\end{equation*}
Let us look at a specific coordinate $i$. Assume $\Pi_{j=1}^{k}p_{ij} \geq \frac{\epsilon}{n}$.
By lagrange multipliers we can know that $\Sigma_{j=1}^{k}p_{ij} \geq k(\frac{\epsilon}{n})^{\frac{1}{k}}$.
Pay attention that:
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros in i}] = k - \Sigma_{j=1}^{k}p_{ij} \leq k \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right)
\end{equation*}
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) = \left(1 - e^{\frac{\log(\frac{\epsilon}{n})}{k}}\right) = \left(1 - e^{\frac{\log(\epsilon) -\log(n)}{k}}\right)
\end{equation*}
If $k = \log(n) \alpha(n)$
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \left(1 - e^{\frac{1}{\alpha(n)}}\right) \sim \frac{1}{\alpha(n)}
\end{equation*}
We can conclude that
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros}] = nk\left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \frac{nk}{\alpha(n)} = n\log(n)
\end{equation*}
So if our protocol just think about the important indexes (the one which has probability to be in the intersection) and just ask every player to send the indexes of his zeros.
\begin{equation*}
    \text{CC} = k + n\log^2(n)
\end{equation*}
\section{$O(k + n\log^2(n))$-Protocol}
Let us pay attention that for big amount of players, the situation is opposite about the distribution of 0-1. If there are a lot of players ($k \in \omega(\log(n))$, most of the bits should be 1's.
\paragraph{Critical Coordinates}
Let us denote a coordinate $i$ critical if:
\begin{equation*}
    \Pr[i \in \bigcap^{k}_{j=1}{X_j}] > \frac{\epsilon}{n}
\end{equation*}
\subsection{The Protocol}
Every one of the players are calculating what are the important coordinates.
Every player (one by one) sends the indexes of his zeros which are in important coordinates to the coordinator.
If there is an important coordinate with no zero - the coordinatore declares intersection.
Otherwise, declare - disjointness.
\paragraph{Communication}
We are going to talk to all of the players $k$.
Now we should ask about the expected numbers of zeros in the important coordinates. We are going to pay $\log(n)$ for each one of them.
The expected number of zeros is:
\begin{equation*}
    nk\left(1-\left(\frac{\epsilon}{n}\right)^{1/k}\right) = nk\left(1-e^{\frac{\log{\frac{\epsilon}{n}}}{k}}\right) \sim nk\frac{\log{\frac{n}{\epsilon}}}{k} = n\log{\frac{n}{\epsilon}}
\end{equation*}
So total expected communication complexity for this protocol is
\begin{equation*}
    k + n\log{\frac{n}{\epsilon}}\log n
\end{equation*}
\paragraph{Error}
We error only when declaring disjoitness and there is an intersection in unimportant coordinate.
\begin{equation*}
    \Pr[\text{ERROR}] = \Pr[\text{i unimportant} \land i \in \bigcap_j X_j] \leq \Sigma_i \Pr[\text{i unimportant} \land i \in \bigcap
    _j X_j] \leq n\frac{\epsilon}{n} = \epsilon
\end{equation*}
\begin{claim}
For $p_1, p_2, ... , p_k \in [0,1]$ \newline
If
\begin{equation*}
    \Pi_{i=1}^{k}p_i \geq \frac{\epsilon}{n}
\end{equation*}
We can know that
\begin{equation*}
    \Sigma_{i=1}^{k}p_i \geq k\left(\frac{\epsilon}{n}\right)^{1/k}
\end{equation*}
\end{claim}
\begin{proof}
By lagrange multipliers let us denote a target function 
\begin{equation*}
    f(x_1, ... , x_k) = \Sigma_{i=1}^{k}x_i
\end{equation*}
A constraint function
\begin{equation*}
    g(x_1, ... , x_k) = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n} 
\end{equation*}
Lagrange function
\begin{equation*}
    \mathcal{L}(x_1, ... , x_k, \lambda) = \Sigma_{i=1}^{k}x_i - \lambda\left(\Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}  \right)
\end{equation*}
\begin{align*}
    \frac{\partial\mathcal{L}}{\partial x_i} = 1 - \lambda\Pi_{j \neq i}x_j  \\
    \frac{\partial\mathcal{L}}{\partial \lambda} = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}
\end{align*}
By (2):
\begin{align*}
    \Pi_{i=1}^{k}x_i = \frac{\epsilon}{n} \\
    1 = \frac{\lambda\frac{\epsilon}{n}}{x_i} \\ 
    x_i = \lambda\frac{\epsilon}{n} \\
    \left(\lambda\frac{\epsilon}{n}\right)^k = \frac{\epsilon}{n} \\
    \lambda = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k} - 1} \\
    x_i = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}} \\
    f_{\text{min}} = k\left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}
\end{align*}
\end{proof}
\section{Lower Bound}
\paragraph{Introduction}
Usually in lower bound using information theory techniques, we firstly move to the problem of disjointness where $n=1$ denoted as $\text{AND}_k$ which is the problem where every player has one bit and they need to answer whether they all got 1 or not. After moving to this problem, we calculate how much information is needed in order to solve it.
\subsection{Direct Sum}
\begin{theorem}
Given a protocol $A$ which solves $DISJ^n_k$ with error $\epsilon$ (for every input). There is a protocol $B$ which solves $AND_k$ with error $\epsilon + ??$ such that \newline
For $X \sim \mu^n, \Pi \sim A(X), X' \sim \mu, \Pi ' \sim B(X') $ \newline
$I(\Pi, X) \geq n I(\Pi ', X')$
\end{theorem}
\paragraph{The Protocol}
For input $X'_i \in \{0, 1\}$, the players use the public randomness in order to find a random index $j \in [k]$ in which they are gonna insert the original input. For the other indexes, every player uses the private randomness in order to sample the other indexes from $\mu$. Then they just run the protocol $A$ for their inputs. They answer the output of $A$. 

\paragraph{Error Analysis}
Our protocol errors in one of two cases: The original protocol errors, or the inputs that the player sampled has intersection and the given doesn't. \newline
$\Pr[Error] \leq \epsilon + \Pr[\underset{j \in [k]}{\bigcap } X^j_{-i} \neq \emptyset] = \epsilon + 1 - (1 - \frac{c}{n})^{n-1} \approx \epsilon + 1 - \frac{1}{\sqrt[c]{e}}$ \newline
Here we have a big obstacle since the error is huge (the one coordinate chance to be disjoint is $\frac{1}{n}$ which alone is problematic since it goes smaller). This hints us that we can not use a constant $\epsilon$ for $AND_k$ and our distribution.

\paragraph{Information Analysis}
$X_j - \text{Input for all of the players in coordinate j}$ \newline
$I(X, \Pi) = \underset{j\in[k]}{\Sigma} I[X_j, \Pi | X_{<j}] \overset{(1)}{\geq } \underset{j\in[k]}{\Sigma} I[X_j, \Pi]$ \newline
$I(X', \Pi ') = \underset{j \in [k]}{\mathbb{E}}[I(X_j, \Pi] = \frac{1}{n}\underset{j\in[k]}{\Sigma} I[X_j, \Pi] \leq \frac{1}{n}I(X, \Pi)$ \newline
(1) - Since the coordinates are independent

\subsection{$AND_k$ Information Cost}
\paragraph{Introduction}
We are going to bound the information cost for a protocol which solves $AND_k$. Our analysis is divided into three blocks: \newline
1 - Using the error of the protocol in order to conclude it must know some information about specific player's input. \newline
2 - Distinguishing this input is leaking some information (specifically KL-Divergence) \newline
3 - Concluding this for the general information cost of the protocol \newline
\paragraph{Definitions}
\begin{definition}
$D(a || b) = \underset{x}{\Sigma}a(x)\log(\frac{a(x)}{b(x)})$
\end{definition}
\paragraph{Protocol properties}
For a transcript $\pi$ and player $i$ there is a function $q_i(\pi, x_i) \in [0,1]$ where \newline
$Pr[\pi | x] = \underset{i}{\Pi}q_i(\pi, x_i )$
\begin{definition}
$\lambda _i (\pi) = \frac{q_i(\pi, 0)}{q_i(\pi, 1)}$
\end{definition}
We should think about this as how much this transcript prefers that $x_i = 0$ over $x_i = 1$.
\paragraph{Part 1 - Protocol Error Analysis}
\begin{lemma}
For any $x \in \{0,1\}^k$, any transcript $\pi$ \newline
Denote $Z(x) = {i \in k | x_i = 0}$ so \newline
$\frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \Pi_{i \in Z(x)} \lambda_i (\pi)$
\end{lemma}
\begin{proof}
$\frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \frac{\Pi_{i \in [k]} q_i (\pi, x_i)}{\Pi_{i \in [k]} q_i (\pi, 1)} = \Pi_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)} = \Pi_{i \in Z(x)} \lambda_i (\pi)$
\end{proof}
Let there be $\alpha \in \mathbb{R}, \alpha \geq 1$ \newline
Let us define a set of transcripts $A = \{\pi | \forall_i \lambda_i (\pi) < \alpha \}$. 
Denote two more sets: $T_1$ - transcripts which are ended with positive answer (there is an intersection). $T_0$ - transcripts which are ended with negative answer (there is no intersection). \newline
Let us pay attention: \newline
$\Pr[A \bigcap T_1 | x = 0^k] \leq \Pr[T_1 | x = 0^k] = \Pr[\text{ERROR} | x = 0^k] \leq \epsilon$ \newline
This is since the right answer for $0^k$ is 0 so $T_1$ is error. \newline 
$\Pr[A \bigcap T_0 | x = 0^k] \leq \alpha ^k \Pr[A \bigcap T_0 | x = 1^k] \leq \alpha ^k \Pr[\text{ERROR} | x = 1^k] \leq \alpha ^k \Pr[T_0| x = 1^k] \leq \alpha ^k \epsilon$ \newline
$\Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k) $ \newline
The set in which the transcripts don't really prefer 0 over 1 isn't very common under $x = 0^k$. \newline
We can also see that $\Pr[A^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k) $ \newline
Pay attention that $A^\complement = \{\pi | \exists_i \lambda_i(\pi) \geq \alpha\}$. \newline
If we define $A_{i}^\complement = \{\pi | \lambda_i(\pi) \geq \alpha\}$, we can see that $A^\complement = \bigcup_{i} A_{i}^\complement$.
Using this fact and union bound, there exists $i$ such that 
\begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
\end{equation*}
That is the finish line of this part. We got a set of transcripts which has nice probability under $x = 0^k$ where the transcripts prefer strongly 0 over 1 for some index. We may use this technique in different ways depends on our distribution of inputs. For small $k$ our distribution give high probability for $0^k$ which is pretty convenient. For other distribution we may want to modify this method. \newline
This is a rough method in order to use the fact that the protocol has to be "biased" in terms of $\lambda_i$ in order to have a low error. \newline
\paragraph{Part 2 - Divergence}
In this part, we are going to show that the set we found in the last part is contributing high enough divergence. This will be enough since $x = 0^k$ has high probability under our distribution. \newline
Let us analyze the connection between $\lambda_i(\pi)$ to its divergence. \newline
Pay attention to this: \newline
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] \overset{(1)}{=} \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi|X_{-i} = 0^{k-1}]} = 
\end{equation*}
\begin{equation*}
    = \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}] + \Pr[\pi | X=0^{i-1}10^{k-i}]\Pr[X_i=1|X_{-i}=0^{k-1}]}
\end{equation*}
\begin{equation*}
 =  \frac{\mu(0)}{\mu(0) + \frac{\Pr[\pi | X=0^{i-1}10^{k-i}]}{\Pr[\pi | X=0^k]}\mu(1)} = \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} 
\end{equation*} 
(1) - by Bayes \newline
We can also conclude: \newline
\begin{equation*}
\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]  = \frac{\frac{\mu(1)}{\lambda_i(\pi)}}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
\end{equation*}
The brave part now (analysing two parts of the divergence): \newline
\paragraph{Part 1 of the divergence:}
$\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)} \log\left( \frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) = -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)}$ \newline
It is true that $\frac{logx}{x} \rightarrow 0$ for $x \rightarrow \infty$ \newline
For small k, $\mu(0) \rightarrow 1$. We can choose big enough $\alpha$ such that for big n, if $\lambda_i(\pi) \geq \alpha$ it is true that: \newline
$\lambda_i(\pi)\mu(0) + \mu(1) \geq \lambda_i(\pi)\mu(0) \geq \frac{\lambda_i(\pi)}{2} \geq \frac{\alpha}{2}$ \newline
$ \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)} < \frac{1}{8}$ \newline
To conclude this part:
$\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)} \log\left( \frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) \geq -\mu(1) \frac{1}{8}$
\paragraph{Part 2 of the divergence:}
$\frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\right) \geq \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\alpha}}\right) = \frac{\mu(0)}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\right) = \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(\frac{1}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) = \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(1 + \frac{\mu(1)(1 -\frac{1}{\alpha})}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) \geq \frac{\mu(0)(\mu(1)(1 -\frac{1}{\alpha}))}{2\left(1 - \mu(1)(1 -\frac{1}{\alpha})\right)^2} \geq \frac{\mu(1)}{4}$ \newline
$\log(1+x) \sim x$ for $x \rightarrow 0$
\paragraph{Conclude Divergence part}
If $\lambda_i(\pi) \geq \pi$, for big enough $n$:
$D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \geq \frac{\mu(1)}{4} $
\paragraph{Part 3 - Information Summary}
\begin{equation*}
    I(X;\Pi) = \overset{k+i}{\underset{j=i+1}{\Sigma } } I[X_{j \% k}, \Pi | X_{<(j \% k)}] \geq I(X_i;\Pi | X_{-i}) = \mathbb{E}_{x, \pi \sim \mu} \left( D\left(\frac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right) \geq
\end{equation*}
\begin{equation*}
    \geq \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq 
\end{equation*}
\begin{equation*}
\mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \left (1 - n^{-\frac{1}{k}} \right)^{k} \cdot \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4} \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\section{Extra}
\paragraph{Binary Distribution}
Let us consider a specific interesting distribution for $n = 2^{k-1}$. We think of $X_i \subseteq \{0, 1, ... , n-1\} $ \newline
For $i \in [k-1]$
  \[
    X_i=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| m_{i-1} = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| m_{i-1} = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
and for $i = k$
  \[
    X_k=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
where $m_i$ is the $i$'th bit of m in binary representation. ($m_i \mathrel{\mathop:}= ( m \mathop{\&} 2^{i-1} ) \gg i-1 $). \\
Let's pay attention for some simple properties. First of all $\forall_i |X_i| = \frac{n}{2}$. Moreover, it doesn't matter if we permute the players, for $i < k, |A_i| = 0.5|A_{i-1}|$ and generally for $i < k, |A_i| = 2^{k-1-i}$. $\Pr[DISJ] = 0.5$. The thing is that this distribution has a little entropy ($k$).
\section{Appendix}
\paragraph{Rounds analysis for upper bound}
\begin{claim}
    For $k \geq 2$ and every $n$: $n^{1 - \frac{1}{k}} - (n-\frac{n^{1/k}}{2})^{1-\frac{1}{k}} \geq 0.25$
\end{claim}
\begin{proof}
    Let us define a function $f(x) = x^{1-\frac{1}{k}}$. We want to analyze $f(n) - f(n-\frac{n^{1/k}}{2})$. By Lagrange's theorem, there exists a $c \in [0, \frac{n^{1/k}}{2}]$ such that $f(n) - f(n-\frac{n^{1/k}}{2}) = \frac{n^{1/k}}{2} f'(n-c)$. By calculation: $f'(x) = (1-\frac{1}{k})x^{-1/k}$. Therefore $f(n) -  f(n-n^{1/k}) = \frac{n^{1/k}}{2} (1-\frac{1}{k}) (n-c)^{-1/k} = \frac{(1-\frac{1}{k})}{2} (\frac{n}{n-c})^{1/k} \geq (1-\frac{1}{k})/2 \geq 0.25$ for $k \geq 2$ 
\end{proof}
\begin{claim}
    Let us define the function $f(n)$ - rounds for the protocol starting with $n$ bits with $k$ players. As described in the protocol we know the equation 
    $f(n) = 1 + f(n - \frac{n^{1/k}}{2})$ \newline
    We claim that $f(n) \in O(n^{1-\frac{1}{k}})$
\end{claim}
\begin{proof}
We should find that $ \exists_{D\in\mathbb{R}} : \forall_{n, k \in \mathbb{N}} : f(n) \leq Dn^{1-\frac{1}{k}}$. Let us prove by induction. The induction base is trivial (we can choose big enough $D$). The induction step: \newline
$f(n) = 1 + f(n - \frac{n^{1/k}}{2}) \overset{(1)}{\leq} 1 + D(n - \frac{n^{1/k}}{2})^{1-\frac{1}{k}} \overset{(2)}{\leq} 1 + D(n^{1-\frac{1}{k}}-0.25) \overset{(3)}{\leq} Dn^{1-\frac{1}{k}}$ \newline
(1) - By induction. \\
(2) - By previous claim. \\
(3) - If we choose $4 \leq D$
\end{proof}

\subsection{TODO}
Add an universe set U. and pseudo code. 
Tough (est?) distribution\newline
Make sure to rethink how does this protocol end \newline
Does it help to permutate the players? \newline
Do we have to tell every player everything? maybe we can get rid of the k factor? I need to ask which one of them may have a critical index... \newline
\end{document}
