\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{paralist}
\usepackage[usenames]{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{appendix}
%\usepackage[boxed,vlined]{algorithm2e}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{authblk}
\usepackage[normalem]{ulem}
\usepackage[ruled,boxed,vlined]{algorithm2e}
\usepackage{float}
\usepackage[colorlinks,urlcolor=black,citecolor=black,linkcolor=black]{hyperref}

\title{Multiparty Disjointness on Product Distributions}
\author{Peleg Kazaz}
\date{August 2019}


%General math style
\newcommand{\fnstyle}[1]{\mathsf{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\coloneq}{:=}
\newcommand{\st}{\medspace \middle| \medspace}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\prob}[1]{\ensuremath{\text{\textsc{#1}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator{\poly}{poly}

%Comments, TODO, etc.
\newcommand{\hide}[1]{ }
\newcommand{\note}[1]{ { \color{blue} #1 } }
\newcommand{\Rnote}[1]{ { \color{magenta} #1 } }
\newcommand{\TODO}[1]{ {\color{red} #1 }}

%Probability
\newcommand{\Ber}{\mathsf{B}}
\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\newcommand{\given}{\medspace \middle| \medspace}
\newcommand{\rv}[1]{\mathbf{#1}}

%Communication & information
\newcommand{\CC}{\mathrm{CC}}
\DeclareMathOperator*{\MI}{I}
\DeclareMathOperator*{\CIC}{CIC}
\DeclareMathOperator{\HH}{H} 


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{invariant}{Invariant}
\newtheorem*{remark}{Remark}

\renewcommand{\include}{\input}

\begin{document}

\maketitle


\section{Introduction}
\Rnote{Not touching this for now}
We are going to consider a sequential point of view for the disjointness problem for k players. Let us imagine a process in which the players go one after another and intersect their sets with the result. Starting with the full set ($[n]$), after the last player plays, we end up with the intersection of all sets. Therefore our question is whether this set is empty or not. Moreover, if the last set is empty, one of the players must subtract a large amount of elements in this process. After he plays we get a set which is in smaller order of magnitude than the one before. The difference between those sets are included in the player's zeros. That is where we have an option to learn them using small amount of communication. 
\subsection{Related Work}
\section{Preliminaries}
\Rnote{Not touching this for now}
\subsection{Notations}
We use the popular notation: We are going to have $k$ players. For $i \in [k], X_i \in \{0,1\}^{n}$ - the $i$'th player's input. Sometimes we are going to think of $X_i$ as a subset of $[n]$ where $X_i = \{j \in [n] | X_{ij} = 1\}$. In this notation, the distjointness problem is to decide whether $\cap^{k}_{j=1}X_j = \emptyset$. \newline
\section{$O(kn^{1-1/k})$-Bit Protocol}
Throughout the protocol, we maintain a set $U \subseteq [n]$ of ``possible intersection elements'', with the property that $\bigcap_i X_i \subseteq U$.
Initially we set $U = \emptyset$.
The protocol operates in \emph{iterations};
in each iteration, our goal is to find a player that can reduce $U$ significantly without using much communication.
If we can find such a player, we use it to reduce $U$, and continue;
if we cannot find such a player, this means that $\bigcap_i X_i \neq \emptyset$, so we can halt and output ``not disjoint''.

Define  sets $[n] = A_0 \supseteq A_1 \supseteq \ldots \supseteq A_k$, as follows:
for each $i = 0,\ldots,k$,
\begin{equation*}
  A_i \coloneq \cap_{j = 1}^i X_j,
\end{equation*}
so that for each $i \in [k]$ we have $A_i = A_{i-1} \cap X_i$.
Our goal in the $\prob{Disj}$ problem is to determine whether or not $A_k = \emptyset$.

\paragraph{Critical players.}
We claim that if $\bigcap_i X_i = \emptyset$, then there is at least one player $i$ such that $A_i$ is significantly smaller than $A_{i - 1}$;
in other words, $X_i$ eliminates a significant fraction of elements from consideration.
\begin{lemma}
  If $\bigcap_i X_i = \emptyset$,
  then there is some player $i \in [k]$
  with
  \begin{align}
    \frac{|A_{i}|}{|A_{i-1}|} < \frac{1}{n^{1/k}} \\
    A_{i} \neq \emptyset
  \end{align}
  \label{lemma:narrow}
\end{lemma}
\begin{proof}
    We first prove that there is such index only with the first property, and than we are going to show that the minimal one of the indexes with the first property has the second one also.

  Let us denote by $j$ the maximal index where 
  \begin{equation*}
    A_{j-1} \neq \emptyset.
  \end{equation*}
  There is such because it is true for $j=1$. \newline
  Because j is maximal, this property doesn't hold for $j+1$ so we know that $j=k$ or $A_j = \emptyset$. But $\bigcap_i X_i = A_k = \emptyset$ so we got $A_j = \emptyset$. 
  Pay attention
  \begin{equation*}
    \frac{|A_j|}{|A_{j-1}|} = \frac{0}{|A_{j-1}|} = 0 < \frac{1}{n^{1/k}}
  \end{equation*}
  So we know that (1) holds for j.
  
  Let us denote $j^\star$ the minimal index which holds (1).
  
  We can see that for all $i \in [j^\star-1]$ we have
  \begin{equation*}
    \frac{|A_i|}{|A_{i-1}|} \geq \frac{1}{n^{1/k}}.
  \end{equation*}
  Then we get that
  \begin{align*}
    \frac{|A_{j^\star}|}{|A_0|} = 
    \frac{|A_{j^\star}|}{|A_{j^\star-1}|} \cdot \frac{|A_{j^\star-1}|}{|A_{j^\star-2}|} \cdot \ldots \cdot \frac{|A_1|}{|A_0|}
    \geq
    \left(  \frac{1}{n^{1/k}} \right)^{j^\star}
    =
    \frac{1}{n^{j^\star/k}}.
  \end{align*}
  Since $|A_0| = n$, this implies that $|A_{j^\star}| \geq n^{1-j^\star/k} \geq 1$. Therefore $A_{j^\star} \neq \emptyset$.
\end{proof}

We say that player $i$ is \emph{critical} if $i$ is the smallest index such that $|A_i| / |A_{i-1}| < 1/n^{1/k}$.
Note that player $i$ does not \emph{know} if it is critical or not, because it does not know $|A_{i-1}|$ (this depends on other players' inputs);
but it can compute the probability that it is critical, and this is what we will use in the protocol.

\paragraph{The protocol.}
As we said, the protocol proceeds in iterations, $r = 1,\ldots,R$
\TODO{replace $R$ with some concrete upper bound on the number of iterations}.

For a player $i$, an iteration $r$,
let
\begin{equation*}
  \gamma_i^r(x_i) \coloneq \Pr_{(\rv{X}_1,\ldots,\rv{X}_k) \sim \mu}\left[ \text{ $i$ is critical in $U_{r-1}$} \land \text{DISJ} \given \rv{X}_i = x_i \right].
\end{equation*}
We say that player $i$ is \emph{significant} if $\gamma_i^r(s) \geq \eps / k$.
\TODO{What is $\eps$?}

In each iteration $r$, the coordinator asks each of the $k$ players whether or not they are significant.
If no player is significant, the coordinator halts and outputs ``not disjoint''.
If there is some significant player, the coordinator chooses the first player $i$ that is significant,
and sends the index $i$ to all players.

Next, using public randomness, the coordinator and the players sample
iid sets $(X_1, \ldots , X_k)^1, \ldots (X_1, \ldots , X_k)^2,\ldots \sim \mu((\rv{X_1}, \ldots , \rv{X_k}))$.
Player $i$ finds the first index $j$ such that he is critical in $(X_1, \ldots X_{i-1}, x_i, X_{i+1}, \ldots , X_k)^j$. In this set, $|A_{i-1}^j \cap x_i| / |A_{i-1}^j| < 1/n^{1/k}$. Now we split $A_{i-1}^j$ to sets of size $\frac{n^{1/k}}{2}$ by order denoted $Z^{r}_1, \ldots , Z^{r}_m$. .By counting argument, one of them doesn't intersect with $A_{i-1}^j \cap x_i$ - let us denote its index by $l$. Player $i$ sends the index $j$ to the coordinator, along with the index $l$ for the right $Z^{r}_l$ to the coordinator.
The coordinator disseminates $j, l$ to all the other players.

Observe that $Z^{r}_l \subseteq \overline{X_i}$,
so the elements of $Z^{r}_l$ can now be removed from the universe:
\begin{equation*}
  U_r \leftarrow U_{r-1} \setminus (Z^{r}_l).
\end{equation*}

\TODO{up to here}



\subsection{Properties of Our Protocol}
Our protocol is using random coins and is described in the coordinator model. It uses an expected value of $O(kn^{1-1/k})$ bits of communication between the players.
\subsection{The Protocol}
Our protocol is divided into rounds. In each round we find out some zeros of some player and then we ignore these indexes from now on and make our universe smaller. \newline
Every round works like this: \newline
The coordinator asks the players one by one whether $\Pr[DISJ \land \text{i is critical} | X_i = x_i \land \text{We got this far in the protocol}] \geq \frac{\epsilon}{k}$. \newline
If all of them answer negative - declare intersection. \newline
Otherwise, choose the first one to take out indexes by the following protocol: \newline
First the coordinator sends his identity ($i$) to all of the players. \newline
Now all of them parse the public random coins as samples of $A_{i-1}$ (Every player knows to parse it). Player $i$ finds a sample in which he is critical. Then he sends its index to the coordinator and after that he sends $A_i (=A_{i-1} \cap x_i)$ index by index. The coordinator sends this information to every other player.
Note that knowing this information they all can deduce $A_i \setminus A_{i-1} \subseteq X^{c}_i$ which are zeros of player $i$. They all take these zeros out (making the universe smaller $U := U \cap (A_i \setminus A_{i-1})$). If $|U| \leq n^{1-1/k}$, everyone sends their inputs to the coordinator which calculate the answer and declares accordingly.
\subsection{Analysis}
\paragraph{Error Analysis}
For each leaf $L$ in our protocol tree (determined by X,Y), we are going to argue that we error for at most $\epsilon$. Therefore in overall we don't error more than $\epsilon$. (We consider the tree given specific random coins) \newline
A leaf $L$ is a round in which we stop. \newline
The only times we error is when we answer due to the probability calculations. The other leaves have 0 error.\newline
$\Pr[ERROR | \text{Stopped at L}] \overset{(1)}{=} \Pr[ERROR \land DISJ | \text{Stopped at L}] \overset{(2)}{=} \sum\limits_{i=1}^k \Pr[ERROR \land \text{DISJ} \land \text{i is critical}| \text{Stopped at L}] \overset{(3)}{=} \sum\limits_{i=1}^k \underset{X_i=x_i}{\mathop{\mathbb{E}}}\Pr[ERROR \land \text{DISJ}\land \text{i is critical}| \text{Stopped at L} \land X_i=x_i] \leq \sum\limits_{i=1}^k \underset{X_i=x_i}{\mathop{\mathbb{E}}}[\frac{\epsilon}{k}] = \sum\limits_{i=1}^k \frac{\epsilon}{k} = \epsilon$ \newline
\newline
$\Pr[ERROR] = \underset{\text{L leaf}}{\mathop{\mathbb{E}}} [\Pr[ERROR | \text{Stopped at L}] ] \leq \epsilon $ \newline
(1) - The protocol errors only when the sets are disjoint (the protocol declares intersection). \newline
(2) - $  \text{DISJ} = \biguplus_{i=1}^{k}\{DISJ \land \text{i is the critical index} \} $ \newline
(3) - Law of total expectation
\paragraph{Communication Analysis}
We analyze round by round. For a single round, firstly we pay at most $k$ bits in order to find if some player has a chance to have critical index. After that the coordinator sends his identity to the others $k\log(k)$ bits. Then the protocol sends the sample index and $A_{i}$ to all of the players. Now we ask what is the expected value of the index. \newline
This is a random variable which has a geometric distribution. \newline
$ \mathop{\mathbb{E}} [J] = \frac{1}{\Pr[DISJ \land \text{i is critical} | X_i = x_i \land \text{We got this far in the protocol}]} \leq \frac{k}{\epsilon}$ \newline
$\mathop{\mathbb{E}} [\log(J)] \leq \log(\mathop{\mathbb{E}} [J]) \leq \log(\frac{k}{\epsilon}) = \log(k) + \log(\frac{1}{\epsilon})$ \newline
Let us define $m : = |A_i|$ for the chosen $A_i$ for the specific round.
Pay attention that $|A_i| \cdot n^{-1/k} \geq |A_{i-1}| \geq n^{1-\frac{i-1}{k}}$ \newline
$m \geq n^{1-\frac{i}{k}} \geq 1$ so $m \neq 0$\newline
We know that $|A_{i-1}| \geq |A_i| \cdot n^{1/k}$. In this round we discovered that $A_{i-1} \setminus A_i$ are zeros for player i. Pay attention $|A_{i-1} \setminus A_i| \overset{(1)}{=} |A_{i-1}| - |A_i| \geq |A_i|(n^{1/k} - 1) = m(n^{1/k} - 1)$ \newline
So in total the cost of the round is $k + k\log(k) + k(\log(k) + \log(\frac{1}{\epsilon})) + k\log(n)$ and we discovered $(n^{1/k} - 1)$ zeros. \newline
$f(n) = k(\log(k) + \log(\frac{1}{\epsilon}) + m\log(n)) + f(n - m(n^{1/k} - 1))$ \newline 
$f(n) \in O(kn^{1-\frac{1}{k}}\log(n))$ \newline
(1) $A_i \subseteq A_{i-1}$
\section{k-n relation analysis}
\paragraph{$k < \log(n)$:}
For this case $k$ is pretty small. meaning $kn^{1/k} \approx n^{1/k}$. So we are pretty tight (our lower bound is almost like our upper one).
\paragraph{$k = \alpha\log(n)$:}
\begin{equation*}
    n^{1/k} = n^{\frac{1}{\alpha\log(n)}} = 2^{\frac{\log(n)}{\alpha\log(n)}} = 2^{1/\alpha}
\end{equation*}
so
\begin{equation*}
    n^{1-1/k} > n/2
\end{equation*}
Moreover, in this scenario:
\begin{equation*}
    p = \frac{1}{n^{1/k}} = \frac{1}{2^{\frac{1}{\alpha}}} 
\end{equation*}
\begin{equation*}
    h(p) = c 
\end{equation*}
\begin{equation*}
    h(\rv{X}) = cnk
\end{equation*}
So our lower bound may also be fine.
\paragraph{$k \in \omega(\log(n))$:}
This is an interesting case. Most of the elements in our input should be 1. 
Let us denote:
\begin{equation*}
    p_{ij} = \Pr[i \in X_j]
\end{equation*}
\begin{equation*}
    \Pr[\neg \text{DISJ}] \leq \Sigma_{i=1}^{n}\Pr[i \in \cap_{j=1}^{k}X_j] = \Sigma_{i=1}^{n}\Pi_{j=1}^{k}p_{ij}
\end{equation*}
Let us look at a specific coordinate $i$. Assume $\Pi_{j=1}^{k}p_{ij} \geq \frac{\epsilon}{n}$.
By lagrange multipliers we can know that $\Sigma_{j=1}^{k}p_{ij} \geq k(\frac{\epsilon}{n})^{\frac{1}{k}}$.
Pay attention that:
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros in i}] = k - \Sigma_{j=1}^{k}p_{ij} \leq k \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right)
\end{equation*}
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) = \left(1 - e^{\frac{\log(\frac{\epsilon}{n})}{k}}\right) = \left(1 - e^{\frac{\log(\epsilon) -\log(n)}{k}}\right)
\end{equation*}
If $k = \log(n) \alpha(n)$
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \left(1 - e^{\frac{1}{\alpha(n)}}\right) \sim \frac{1}{\alpha(n)}
\end{equation*}
We can conclude that
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros}] = nk\left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \frac{nk}{\alpha(n)} = n\log(n)
\end{equation*}
So if our protocol just think about the important indexes (the one which has probability to be in the intersection) and just ask every player to send the indexes of his zeros.
\begin{equation*}
    \text{CC} = k + n\log^2(n)
\end{equation*}
\section{$O(k + n\log^2(n))$-Protocol}
Let us pay attention that for big amount of players, the situation is opposite about the distribution of 0-1. If there are a lot of players ($k \in \omega(\log(n))$, most of the bits should be 1's.
\paragraph{Critical Coordinates}
Let us denote a coordinate $i$ critical if:
\begin{equation*}
    \Pr[i \in \bigcap^{k}_{j=1}{X_j}] > \frac{\epsilon}{n}
\end{equation*}
\subsection{The Protocol}
Every one of the players are calculating what are the important coordinates.
Every player (one by one) sends the indexes of his zeros which are in important coordinates to the coordinator.
If there is an important coordinate with no zero - the coordinatore declares intersection.
Otherwise, declare - disjointness.
\paragraph{Communication}
We are going to talk to all of the players $k$.
Now we should ask about the expected numbers of zeros in the important coordinates. We are going to pay $\log(n)$ for each one of them.
The expected number of zeros is:
\begin{equation*}
    nk\left(1-\left(\frac{\epsilon}{n}\right)^{1/k}\right) = nk\left(1-e^{\frac{\log{\frac{\epsilon}{n}}}{k}}\right) \sim nk\frac{\log{\frac{n}{\epsilon}}}{k} = n\log{\frac{n}{\epsilon}}
\end{equation*}
So total expected communication complexity for this protocol is
\begin{equation*}
    k + n\log{\frac{n}{\epsilon}}\log n
\end{equation*}
\paragraph{Error}
We error only when declaring disjoitness and there is an intersection in unimportant coordinate.
\begin{equation*}
    \Pr[\text{ERROR}] = \Pr[\text{i unimportant} \land i \in \bigcap_j X_j] \leq \Sigma_i \Pr[\text{i unimportant} \land i \in \bigcap
    _j X_j] \leq n\frac{\epsilon}{n} = \epsilon
\end{equation*}
\begin{claim}
For $p_1, p_2, ... , p_k \in [0,1]$ \newline
If
\begin{equation*}
    \Pi_{i=1}^{k}p_i \geq \frac{\epsilon}{n}
\end{equation*}
We can know that
\begin{equation*}
    \Sigma_{i=1}^{k}p_i \geq k\left(\frac{\epsilon}{n}\right)^{1/k}
\end{equation*}
\end{claim}
\begin{proof}
By lagrange multipliers let us denote a target function 
\begin{equation*}
    f(x_1, ... , x_k) = \Sigma_{i=1}^{k}x_i
\end{equation*}
A constraint function
\begin{equation*}
    g(x_1, ... , x_k) = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n} 
\end{equation*}
Lagrange function
\begin{equation*}
    \mathcal{L}(x_1, ... , x_k, \lambda) = \Sigma_{i=1}^{k}x_i - \lambda\left(\Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}  \right)
\end{equation*}
\begin{align*}
    \frac{\partial\mathcal{L}}{\partial x_i} = 1 - \lambda\Pi_{j \neq i}x_j  \\
    \frac{\partial\mathcal{L}}{\partial \lambda} = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}
\end{align*}
By (2):
\begin{align*}
    \Pi_{i=1}^{k}x_i = \frac{\epsilon}{n} \\
    1 = \frac{\lambda\frac{\epsilon}{n}}{x_i} \\ 
    x_i = \lambda\frac{\epsilon}{n} \\
    \left(\lambda\frac{\epsilon}{n}\right)^k = \frac{\epsilon}{n} \\
    \lambda = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k} - 1} \\
    x_i = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}} \\
    f_{\text{min}} = k\left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}
\end{align*}
\end{proof}
\section{Extra}
\paragraph{Binary Distribution}
Let us consider a specific interesting distribution for $n = 2^{k-1}$. We think of $X_i \subseteq \{0, 1, ... , n-1\} $ \newline
For $i \in [k-1]$
  \[
    X_i=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| m_{i-1} = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| m_{i-1} = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
and for $i = k$
  \[
    X_k=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
where $m_i$ is the $i$'th bit of m in binary representation. ($m_i \mathrel{\mathop:}= ( m \mathop{\&} 2^{i-1} ) \gg i-1 $). \\
Let's pay attention for some simple properties. First of all $\forall_i |X_i| = \frac{n}{2}$. Moreover, it doesn't matter if we permute the players, for $i < k, |A_i| = 0.5|A_{i-1}|$ and generally for $i < k, |A_i| = 2^{k-1-i}$. $\Pr[DISJ] = 0.5$. The thing is that this distribution has a little entropy ($k$).
\section{Appendix}
\paragraph{Proof for $f(n) \in O(kn^{1-1/k})$}
\begin{claim} 
For $f(n) = k(\log(k) + \log(\frac{1}{\epsilon}) + m) + f(n - m(n^{1/k} - 1))$, it is true that $f(n) \in O(kn^{1-1/k})$ (when thinking of $\epsilon$ as a constant). \newline
\end{claim}
\begin{proof}
We should find that $ \exists_{D\in\mathbb{R}} : \forall_{n \in \mathbb{N}} : f(n) \leq Dkn^{1-1/k}$. Let us prove by induction. The induction base is trivial (we can choose big enough $D$). The induction step: \newline
We can think of $\epsilon$ as a constant. \newline
$f(n) = k(\log(k) + m) + f(n - m(n^{1/k} - 1)) \leq k(\log(k) + m) + Dk(n - m(n^{1/k} - 1))^{1-1/k} \overset{?}{\leq} Dkn^{1-1/k}$ \newline

\begin{align*}
  k(\log(k) + m) &\overset{?}{\leq} Dkn^{1-1/k} -  Dk(n - m(n^{1/k} - 1))^{1-1/k}\\
  (\log(k) + m) &\overset{?}{\leq} D(n^{1-1/k} -  (n - m(n^{1/k} - 1))^{1-1/k})\\
  \frac{\log(k) + m}{n^{1-1/k} -  (n - m(n^{1/k} - 1))^{1-1/k}} & \overset{?}{\leq} D \\
\end{align*}
\newline
Maybe we can just say that the worse case is when $m = 1$.
In this case we have $n^{1-\frac{1}{k}}$ rounds where each one costs $k\log(k)$ so $kn^{1-\frac{1}{k}}\log(k)$
\end{proof}

\begin{claim}
\begin{equation*}
    \lim_{n \rightarrow \infty} n^{1-\frac{1}{k}} - \left( n - n^{\frac{1}{k}} \right)^{1-\frac{1}{k}} = 1-\frac{1}{k}
\end{equation*}
\end{claim}
\begin{proof}
$$\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}} - \left( n - n^{\frac{1}{k}} \right)^{1-\frac{1}{k}} = 
\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}} \left(1 - \frac{\left( n - n^{\frac{1}{k}} \right)^{1-\frac{1}{k}}}{n^{1-\frac{1}{k}}}\right) = 
\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}}\left(1 - \left( \frac{n - n^{\frac{1}{k}} }{n}\right)^{1-\frac{1}{k}}\right) = $$ \newline
$$
\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}}\left(1 - \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{1-\frac{1}{k}}\right) =
\frac{1}{\lim_{n \rightarrow \infty} \frac{n^{-1+\frac{1}{k}}}{1 - \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{1-\frac{1}{k}}}} \overset{(L)}{=} $$ \newline
$$\frac{1}{\lim_{n \rightarrow \infty} \frac{\left(-1+\frac{1}{k}\right)n^{-2+\frac{1}{k}}}{-\left(1-\frac{1}{k}\right) \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{-\frac{1}{k}} \cdot (-1) \cdot \left(-1+\frac{1}{k}\right)n^{-2+\frac{1}{k}}}} =
\frac{1}{\lim_{n \rightarrow \infty} \frac{1}{\left(1-\frac{1}{k}\right) \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{-\frac{1}{k}}}} =$$ \newline
$$
\frac{1}{\frac{1}{\left(1-\frac{1}{k}\right) \left( 1 - 0\right)^{-\frac{1}{k}}}} = \frac{1}{\frac{1}{\left(1-\frac{1}{k}\right)}} = 1-\frac{1}{k}. 
$$

(L) - L'Hôpital's rule
\end{proof}

\subsection{TODO}
Add an universe set U. and pseudo code. 
Tough (est?) distribution\newline
Make sure to rethink how does this protocol end \newline
Does it help to permutate the players? \newline
Do we have to tell every player everything? maybe we can get rid of the k factor? I need to ask which one of them may have a critical index... \newline
\end{document}
