\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{times}
\usepackage{paralist}
\usepackage[usenames]{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{authblk}
\usepackage[normalem]{ulem}
\usepackage[ruled,boxed,vlined]{algorithm2e}
\usepackage{float}
\usepackage[colorlinks,urlcolor=black,citecolor=black,linkcolor=black]{hyperref}

\title{Multiparty Disjointness on Product Distributions}
\author{Peleg Kazaz}
\date{June 2020}


%General math style
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\fnstyle}[1]{\mathsf{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\coloneq}{:=}
\newcommand{\st}{\medspace \middle| \medspace}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\prob}[1]{\ensuremath{\text{\textsc{#1}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator{\poly}{poly}

%Comments, TODO, etc.
\newcommand{\hide}[1]{ }
\newcommand{\note}[1]{ { \color{blue} #1 } }
\newcommand{\Rnote}[1]{ { \color{magenta} #1 } }
\newcommand{\TODO}[1]{ {\color{red} #1 }}

%Probability
\newcommand{\Ber}{\mathsf{B}}
\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\newcommand{\given}{\medspace \middle| \medspace}
\newcommand{\rv}[1]{\mathbf{#1}}

%Communication & information
\newcommand{\CC}{\mathrm{CC}}
\DeclareMathOperator*{\MI}{I}
\DeclareMathOperator*{\CIC}{CIC}
\DeclareMathOperator{\HH}{H} 


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{invariant}{Invariant}
\newtheorem*{remark}{Remark}

\renewcommand{\include}{\input}

\begin{document}

\maketitle


\section{Introduction}
Yao's two-party communication complexity is a well-studied model. The most simple settings is the deterministic one. We want to find an algorithm which gets the exact solution for every input. This model has two somewhat similar extensions. The first one is the randomized settings. In this model, every player gets a random string (private or public) and can use it in the protocol. In this model we want the protocol to solve the problem with error up to $\epsilon$ (on the randomized strings). \newline
Another model is when the inputs themselves are randomized. In this model, we need to design a protocol which solves the problem with error up to $\epsilon$ for every distribution on the inputs. Another interesting problem is to design a protocol which solves the problem for every product distribution (in which the players' inputs are independent. \newline
One of the most important problems (sometimes called "mother of problems") is the Set Disjointness problem. In this problem, every player gets a n-bit array and they are asked whether there is an index $i$ where the $i$'th bit is turned on in all of their inputs. \newline
This problem has been studied thoroughly on different models. It has been proved that every randomized protocol must use $\Omega(n)$ in order to achieve constant error bound from $\frac{1}{2}$.  Also it has been proved that the distributional communication complexity of this problem is also $\Omega(n)$ using the corruption methods. Another interesting question is what happens when we limit ourselves to product distribution. In this case a tight bound of $\Theta(\sqrt(n))$ has been proved but only for two players. \newline
In this paper, we tried to expand this bound for every number of player $k$. We have succeeded to prove the upper bound of $O(kn^{\frac{1}{k}})$. 
In the two-player protocol, Bob reveals a large amount of his zeros to Alice using the public randomness and the fact that the inputs are drawn from a known product distribution. This method is not easy to generalize since every player may eliminate $\frac{n}{k}$ zeros 
\subsection{Notations}
\Rnote{Not touching this for now}
We use the popular notation: We are going to have $k$ players. For $i \in [k], X_i \in \{0,1\}^{n}$ - the $i$'th player's input. Sometimes we are going to think of $X_i$ as a subset of $[n]$ where $X_i = \{j \in [n] | X_{ij} = 1\}$. In this notation, the distjointness problem is to decide whether $\cap^{k}_{j=1}X_j = \emptyset$. \newline

\begin{lemma}
  Suppose we have a set $A \subseteq [N]$ which is known to all players, and a set $A_i \subseteq A$ that is known to some player $i \in [k]$.
  Let $m, n \in \nat$ be such that $m \leq |A| \leq n$, and assume further that $|A_i| \leq |A|/m$.
  Then there is a deterministic protocol where only player $i$ speaks,
  that allows all players to learn a set $B \subseteq A \setminus A_i$
  of size $|B| \geq m/2$,
  using $\log n$ bits of communication.
  \label{lemma:reveal_zeroes}
\end{lemma}
\begin{proof}
  Fix in advance a partition of $A$ into sets $B_1,\ldots,B_{\ell}$,
  where $|B_j| = m/2$ for each $j = 1,\ldots,\ell-1$,
  and the last set has size $m/2 \leq B_{\ell} \leq m - 1$.
  Note that we have $\ell > |A_i|$:
  each $B_j$ comprises $m/2$ elements,
  \begin{equation*}
    \ell \geq \frac{|A|}{m/2} - 1 > \frac{|A|}{m} \geq |A_i|.
  \end{equation*}
  Therefore, there exists $j \in [\ell]$ such that $B_j \cap A_i = \emptyset$:
  in order for $A_i$ to intersect all $\ell$ sets $B_1,\ldots,B_{\ell}$,
  we would need to have $|A_i| \geq \ell$.

  The protocol is therefore to have player $i$ send the index $j \in [\ell]$ of a set $B_j$
  such that $A_i \cap B_j = \emptyset$.
  The requirements are satisfied: we have $B_j \subseteq A \setminus A_i$,
  and $|B_j| \geq m / 2$.
  Specifying the index $j$ requires $\log \ell \leq \log n$ bits.
  
\end{proof}



\subsection{Overview of the Protocol}
Throughout the protocol, we maintain a \emph{universe}, $U \subseteq [N]$,
with the property that $\bigcap_i x_i \subseteq U$.
Initially, $U = [N]$.

The protocol operates in \emph{iterations}, where each iteration has two possible outcomes:
\begin{enumerate}[I.]
  \item We eliminate at least a $1/\Theta(|U|^{1/k})$-fraction of elements from $U$, or
  \item We halt and output 0. This output is not always correct, but it is correct with high probability over the inputs.
\end{enumerate}
After at most $O(N^{1 - 1/k})$ iterations, if we have not yet halted,
then $|U| < c$ for some constant \TODO{update condition}. At this point each player $i$ announces $x_i \cap U$,
and then players output 1 iff $\bigcap_{i = 1}^k (x_i \cap U) = \emptyset$.


\paragraph{Finding a player that can reduce the universe size.}
Consider a process where we start with the current universe $U$,
and gradually intersect it with more and more players' inputs, 
obtaining a sequence of sets $A_0(U), A_1(U),\ldots,A_k(U) \subseteq U$,
where $A_i(U) = \bigcap_{j \leq i} x_j$ (and in particular, $A_0(U) = U$).
We omit the universe $U$ from our notation when it is clear from the context.

Since $\bigcap_{i = 1}^k x_i \subseteq A_k \subseteq \ldots \subseteq A_0$,
any element that is missing from some $A_i$ is definitely not in the intersection.
Our goal is to try to find two consecutive sets $A_i, A_{i+1}$ such that $A_{i+1}$ is \emph{significantly smaller} than $A_i$,
and then use this fact to eliminate many elements from consideration, reducing the universe size.

\begin{definition}
  With respect to a universe $U \subseteq [N]$ of size $n = |U| > 0$ and inputs $x_1,\ldots,x_k \subseteq [N]$,
  we say that an index $i \in [k]$ is \emph{good} if
  \begin{enumerate}
    \item $|A_i(U)| / |A_{i-1}(U)| < 1/n^{1/k}$, and
    \item $|A_{i-1}(U)| \geq n^{1-(i-1)/k}$.
  \end{enumerate}
  \label{def:good}
\end{definition}

\begin{lemma}
  Fix a universe $U \subseteq [N]$ of size $|U| = n > 0$ and inputs $x_1,\ldots,x_k \subseteq [N]$,
  and suppose that $\bigcap_{i = 1}^k x_i \cap U = \emptyset$.
  Then there is some good index $i \in [k]$.
  \label{lemma:narrow}
\end{lemma}
\begin{proof}
  We first prove that there is an index $j \in [k]$ that satisfies the first property in Def.~\ref{def:good}, and then we show
  that the minimal such index $j$ also has the second property.

  Let $j \in [k]$ be the maximal index such that 
  \begin{equation*}
    A_{j-1} \neq \emptyset.
  \end{equation*}
  There is such a $j$, because $A_0 = U \neq \emptyset$.

  Observe that $A_{j} = \emptyset$:
  if $j < k$ then this is immediate (otherwise $j$ would not be maximal),
  and
  if $j = k$, then $A_j = A_k = U \cap \bigcap_{i=1}^k x_i = \emptyset$ by assumption.
  Thus,
  \begin{equation*}
    \frac{|A_j|}{|A_{j-1}|} = \frac{0}{|A_{j-1}|} = 0 < \frac{1}{n^{1/k}},
  \end{equation*}
  so we know that (1) holds for $j$.


  Let $j^{\ast}$ denote the minimal index in $[k]$ that satisfies (1).
  Because of its minimality, for all $i \in [j^\ast-1]$ we have
  \begin{equation*}
    \frac{|A_i|}{|A_{i-1}|} \geq \frac{1}{n^{1/k}}.
  \end{equation*}
  Therefore,
  \begin{align*}
    \frac{|A_{j^\ast - 1}|}{|A_0|} = 
    \frac{|A_{j^\ast - 1}|}{|A_{j^\ast-2}|} \cdot \frac{|A_{j^\ast-2}|}{|A_{j^\ast-3}|} \cdot \ldots \cdot \frac{|A_1|}{|A_0|}
    \geq
    \left(  \frac{1}{n^{1/k}} \right)^{j^\ast - 1}
    =
    \frac{1}{n^{\frac{j^\ast - 1}{k}}}.
  \end{align*}
  Since $|A_0| = n$, 
  \begin{align*}
    |A_{j^\ast - 1}| \geq n^{1 - \frac{j^\ast-1}{k}}.
  \end{align*}
\end{proof}

Let $D_i(U)$ be an indicator for the event that $\bigcap_{i = 1}^k x_i = \emptyset$ and in addition $i$ is a good index
with respect to the universe $U$.
The lemma implies that for any $U \subseteq [n]$ and inputs $x_1,\ldots,x_n$ such that $\bigcap_{i = 1}^k x_i \subseteq U$,
we have
$\bigcap_{i = 1}^k x_i = \emptyset$
iff $\bigvee_{i = 1}^k D_i(U) = 1$.
As usual, we omit the universe $U$ from our notation where possible.

\paragraph{Significant players.}
A player $i \in [k]$ does not know whether or not $i$ is good,
as this depends on the inputs of the other players.
However, player $i$ can compute the \emph{probability} that the conditions hold:
define
\begin{equation*}
  \gamma_i = \gamma_i(x_i, U) \coloneq \Pr_{(\rv{X}_{-i}) \sim \mu}\left[ D_i(U) \given \rv{X}_i = x_i \right].
\end{equation*}
We say that player $i$ is \emph{significant} if $\gamma_i \geq \eps / (kN^{1-1/k})$.

\subsection{The Protocol}

For a given product distribution $\mu : \left(\set{0,1}^N\right)^k \rightarrow [0,1]$, we describe a protocol that errs with probability $\epsilon$.
Our protocol uses public randomness and has $\tilde{O}(kN^{1-\frac{1}{k}})$ bits of expected communication.

Initially, $U = [N]$, and $n = |U| = N$.
The protocol proceeds in iterations, where in each iteration, we either halt and output 0,
or we reduce the size of the universe by a $1/N^{1/k}$-fraction.
Eventually, when $|U| \leq N^{1-1/k}$, each player $i$ announces $x_i \cap U$.
We then halt and output 1 if $\bigcap_{i = 1}^k (x_i \cap U) = \emptyset$, or 0 otherwise.

Each iteration proceeds as follows:
\begin{enumerate}[(1)]
  \item We go through the players, and each player announces whether or not it is significant, by writing a single bit on the board.
  \item If no player is significant, we halt and output 0.
  \item Otherwise, let $i \in [k]$ be the index of the first significant player. Using public randomness,
    we sample $\ell = $ \TODO{how many} sets $A_{i-1}^{(1)},\ldots,$\TODO{how many}
    from the distribution of $\rv{A}_{i-1}(U)$ \TODO{conditioned on anything?}.
  \item Player $i$ announces the index $j$ of the first set $A_{i-1}^{(j)}$ such that 

\end{enumerate}<++>

\begin{enumerate}[(1)]
  \item Termination Condition: If $|U| \leq N^{1 - 1/k}$, each player $i$ sends $x_i$ to the coordinator, who computes and outputs the answer. 
  \item Otherwise, the coordinator asks each player $i$ whether $i$ is significant in $U$ (i.e., whether $\gamma_i(x_i, U) \geq \eps / kN^{1-\frac{1}{k}}$).
  \item If no player is significant, the coordinator outputs ``not disjoint''. Otherwise, let $i$ be the
    first significant player. The coordinator informs every player that player $i$ has been selected.
  \item The coordinator and the players use the public randomness to sample \TODO{how many needed?}
    sets $A_{i-1}^{(1)},\ldots$ from the distribution of $\rv{A}_{i-1}(U)$.
  \item Player $i$ finds the index $j$ of the first set $A_{i-1}^{(j)}(U)$ such that player $i$ is critical.
    It sends the index $j$ to the coordinator, who forwards it to the other players.
  \item The participants use the protocol from lemma 2.1 in order to discover $B \subseteq A_{i-1}^{(j)} \setminus X_i $ where $ |B| > \frac{n^{1/k}}{2}$ 
  \item All participants set:
    \begin{itemize}
      \item $U \leftarrow U \setminus B$,
      \item $n \leftarrow |U|$,
      \item $x_i \leftarrow x_i \cap U$,
      \item $\mu \leftarrow \mu $ (isn't updated)
    \end{itemize}
\end{enumerate}

%\begin{algorithm}[H]
  %\SetAlgoLined
  %initialization\;
  %\While{not at end of this document}
  %{
    %read current\;
    %\eIf{understand}{go to next section\;current section becomes this one\;}
    %{go back to the beginning of current section\;}
  %}
  %\caption{How to write algorithms}
%\end{algorithm}

%In each iteration $r$, the coordinator asks each of the $k$ players whether or not they are significant.
%If no player is significant, the coordinator halts and outputs ``not disjoint''.
%If there is some significant player, the coordinator chooses the first player $i$ that is significant,
%and sends the index $i$ to all players.
%
%Next, using public randomness, the coordinator and the players sample infinitely many \TODO{(figure out how many really)}
%iid inputs $(X_1, \ldots , X_k)^1, \ldots (X_1, \ldots , X_k)^2,\ldots \sim \mu((\rv{X_1}, \ldots , \rv{X_k}))$.
%Player $i$ finds the first index $j$ such that he is critical in $(X_1, \ldots X_{i-1}, x_i, X_{i+1}, \ldots , X_k)^j$,
%sends $j$ to the coordinator, and then
%
%and uses 
%We use this sample in order to find elements in $[n] \setminus x_i$ as following: Critical property claims that $|A_{i-1}^j \cap x_i| / |A_{i-1}^j| < 1/n_{r-1}^{1/k}$. Now we split $A_{i-1}^j$ to sets of size $\frac{n_{r-1}^{1/k}}{2}$ by order denoted $Z^{r}_1, \ldots , Z^{r}_m$. .By counting argument, one of them doesn't intersect with $A_{i-1}^j \cap x_i$ - let us denote its index by $l$. Player $i$ sends the index $j$ to the coordinator, along with the index $l$ for the right $Z^{r}_l$ to the coordinator.
%The coordinator disseminates $j, l$ to all the other players.
%
%Observe that $Z^{r}_l \subseteq \overline{X_i}$,
%so the elements of $Z^{r}_l$ can now be removed from the universe:
%\begin{equation*}
  %U \leftarrow U_{r-1} \setminus (Z^{r}_l).
%\end{equation*}

%If $n_r \leq n^{1-\frac{1}{k}}$, every player sends his input to the coordinator which calculate the disjointness and sends the output to every player.

%\paragraph{Clarifications}
%We strongly use the fact that this is a product distribution where everyone can sample the inputs and parse them and the critical player's input doesn't affect the distribution. \newline
%For specific inputs, one of the player must be critical if there is an intersection (as proved in the lemma) but it is not easy to know which one of them is critical. 
%\TODO{up to here}



\subsection{Properties of Our Protocol}
Our protocol is using random coins and is described in the coordinator model. It uses an expected value of $O(kn^{1-\frac{1}{k}})$ bits of communication between the players.
\subsection{Analysis}
\paragraph{Error Analysis}
Let there be a round $r$. In this round, we error only if there was no significant player (in $U_r$) and therefore the coordinator answered "not disjoint" but actually the inputs were disjoint. \newline

$\mu(\text{Err in round r}) \leq \mu(DISJ \land \forall_i \text{ i isn't signficant})$ \newline

Using the definition of "significant player" we may rewrite as: \newline

$\mu(\text{Err in round r}) \leq \mu(DISJ \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

As proved in lemma 1.2, if the inputs are disjoint, there is a good player. $DISJ = \bigcup_{j = 1}^k D_j(U_r)$ \newline

$ \mu(DISJ \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) = \mu(\bigcup_{j = 1}^k D_j(U_r) \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

Using union bound we may consider the sum of these probabilities. \newline

$ \mu(\bigcup_{j = 1}^k D_j(U_r) \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) \leq \Sigma_j \mu(D_j(U_r) \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

For every element in the sum, we relax the event the only the specific player is good. \newline

$\Sigma_j \mu(D_j(U_r) \land \forall_i \gamma_i(X_i, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) \leq \Sigma_j \mu(D_j(U_r) \land \gamma_j(X_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}})$ \newline

At this point we stop the calculation and consider a specific element in this sum. Every element is actually the event in which a player is good but not significant or in other words the player is good but the probability that he is good is small. Using this definition it is pretty clear that the probability of this event is small. \newline
Let us consider the expectation of this probability over the player's input. \newline

$\mu(D_j(U_r) \land \gamma_j(X_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) = \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(D_j(U_r) \land \gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}} | X_j = x_j)]$ \newline

Now the player's input is constant and it's significance is also a constant. \newline

$\underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(D_j(U_r) \land \gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}} | X_j = x_j)] = \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(D_j(U_r) | X_j = x_j) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}]$ \newline

Recall the definition of significance player $\gamma_i(x_i, U) \coloneq \Pr_{(\rv{X}_{-i}) \sim \mu}\left[D_i (U) \given \rv{X}_i = x_i \right]$ \newline

$\underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\mu(D_j(U_r) | X_j = x_j) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}] = \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\gamma_j(x_j, U_r) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}]$ \newline

Now we got an expectation over a non-negative r.v ($\gamma_j(x_j, U_r)$) multiplied by the indicator of the event it is in smaller than $\frac{\epsilon}{kN^{1-\frac{1}{k}}}$. We may replace the r.v with $\frac{\epsilon}{kN^{1-\frac{1}{k}}}$. \newline

$\underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\gamma_j(x_j, U_r) \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}] \leq \underset{x_j \sim \mu}{\mathop{\mathbb{E}}}[\frac{\epsilon}{kN^{1-\frac{1}{k}}} \mathbbm{1}_{\gamma_j(x_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}}] \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}$ \newline

In this point we finished considering the specific element (case in which a player is good but not significant) and we can resume the calculation of the whole error in round $r$.  \newline

$\Sigma_j \mu(DISJ_j(U_r) \land \gamma_j(X_j, U_r) \leq \frac{\epsilon}{kN^{1-\frac{1}{k}}}) \leq \Sigma_j \frac{\epsilon}{kN^{1-\frac{1}{k}}} = k\frac{\epsilon}{kN^{1-\frac{1}{k}}} = \frac{\epsilon}{N^{1-\frac{1}{k}}}$
\newline

To conclude, we just proved that the error in a specific round $r$:
$\mu(\text{Err in round r}) \leq \frac{\epsilon}{N^{1-\frac{1}{k}}}$.\newline

Now we are consider the overall error of the protocol (summing over all rounds):

$\mu(Err) = \Sigma_{r} \mu(\text{Err in round r}) \leq \#\{\text{number of rounds}\} \frac{\epsilon}{N^{1-\frac{1}{k}}}$. \newline
In our protocol there are $O(N^{1-\frac{1}{k}})$ so
$\mu(Err) \in O(\epsilon)$

\paragraph{Communication Analysis}
Let us first analyze the expected communication for a single round: \newline

1. Every player sends a bit whether he is significant or not - $k$ bits.\newline

2. The coordinator informs everyone who is the chosen significant - $klogk$ bits. \newline

3. The index $j$ of $A_{i-1}^{(j)}$ is sent. \newline
This is a random variable which has a geometric distribution since $A_{i-1}^{(j)}$ are independent. \newline

Recall that $j$ is chosen where $i$ is critical in $U$. \newline
Therefore we are looking for the following probability:
$\Pr[\text{i is critical in U}| X_i = x_i]$ \newline

Since $DISJ_i(U) = DISJ \cap \{\text{i is critical in U}\}$ (by definition), $\{\text{i is critical in U}\} \subseteq DISJ_i$ which leads to \newline
$\Pr[\text{i is critical in U}| X_i = x_i] \geq \Pr[DISJ_i(U) | X_i = x_i]$ \newline

By definition of $\gamma_i(x_i, U)$: \newline
$\Pr[\text{i is critical in U}| X_i = x_i] \geq \Pr[DISJ_i(U) | X_i = x_i] = \gamma_i(x_i, U) $\newline

Since i is significant: \newline
$\gamma_i(x_i, U) \geq \frac{\epsilon}{N^{1-\frac{1}{k}}}$ \newline

To conclude: $\Pr[\text{i is critical in U}| X_i = x_i] \geq \frac{\epsilon}{N^{1-\frac{1}{k}}}$. \newline

Now we can calculate easily the size of j: \newline
$ \mathop{\mathbb{E}} [J] = \frac{1}{\Pr[\text{i is critical in U}| X_i = x_i]} \leq \frac{1}{\gamma_i(x_i, U)} \leq \frac{kN^{1-\frac{1}{k}}}{\epsilon}$ \newline
By Jensen's inequality, $\mathop{\mathbb{E}} [\log(J)] \leq \log(\mathop{\mathbb{E}} [J]) \leq \log(\frac{kN^{1-\frac{1}{k}}}{\epsilon}) \leq \log(N) + \log(k) + \log(\frac{1}{\epsilon})$.  \newline

4. The player uses lemma 2.1 - $k\log(N)$ bits.

So in total the cost of the round is $k + k\log(k) + k(\log(N) + \log(k) + \log(\frac{1}{\epsilon})) + k\log(N) \in O(k(\log(N) + \log(\frac{1}{\epsilon}))$.

As can be seen in the appendix - there are $O(N^{1-\frac{1}{k}})$ rounds (since in every round we eliminate $n^{\frac{1}{k}}/2$). \newline
For the round where we terminate, we pay at most $kN^{1-\frac{1}{k}} \log(N)$. \newline
So the total communication cost is  $O(kN^{1-\frac{1}{k}}(\log(\frac{1}{\epsilon}) + \log(N)))$.
\section{k-n relation analysis}
\paragraph{$k < \log(n)$:}
For this case $k$ is pretty small. meaning $kn^{1-1/k}\log(n) \approx n^{1-1/k}$. 
\paragraph{$k = \alpha\log(n)$:}
\begin{equation*}
    n^{1/k} = n^{\frac{1}{\alpha\log(n)}} = 2^{\frac{\log(n)}{\alpha\log(n)}} = 2^{1/\alpha}
\end{equation*}
so
\begin{equation*}
    n^{1-\frac{1}{k}} > n/2
\end{equation*}
Moreover, in this scenario:
\begin{equation*}
    p = \frac{1}{n^{1/k}} = \frac{1}{2^{\frac{1}{\alpha}}} 
\end{equation*}
\begin{equation*}
    h(p) = c 
\end{equation*}
\begin{equation*}
    h(\rv{X}) = cnk
\end{equation*}
So the entropy for our distribution is indeed $\Theta(nk)$.
\paragraph{$k \in \omega(\log(n))$:}
This is an interesting case. Most of the elements in our input should be 1. 
Let us denote:
\begin{equation*}
    p_{ij} = \Pr[i \in X_j]
\end{equation*}
\begin{equation*}
    \Pr[\neg \text{DISJ}] \leq \Sigma_{i=1}^{n}\Pr[i \in \cap_{j=1}^{k}X_j] = \Sigma_{i=1}^{n}\Pi_{j=1}^{k}p_{ij}
\end{equation*}
Let us look at a specific coordinate $i$. Assume $\Pi_{j=1}^{k}p_{ij} \geq \frac{\epsilon}{n}$.
By lagrange multipliers we can know that $\Sigma_{j=1}^{k}p_{ij} \geq k(\frac{\epsilon}{n})^{\frac{1}{k}}$.
Pay attention that:
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros in i}] = k - \Sigma_{j=1}^{k}p_{ij} \leq k \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right)
\end{equation*}
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) = \left(1 - e^{\frac{\log(\frac{\epsilon}{n})}{k}}\right) = \left(1 - e^{\frac{\log(\epsilon) -\log(n)}{k}}\right)
\end{equation*}
If $k = \log(n) \alpha(n)$
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \left(1 - e^{\frac{1}{\alpha(n)}}\right) \sim \frac{1}{\alpha(n)}
\end{equation*}
We can conclude that
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros}] = nk\left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \frac{nk}{\alpha(n)} = n\log(n)
\end{equation*}
So if our protocol just think about the important indexes (the one which has probability to be in the intersection) and just ask every player to send the indexes of his zeros.
\begin{equation*}
    \text{CC} = k + n\log^2(n)
\end{equation*}
\section{$O(k + n\log^2(n))$-Protocol}
Let us pay attention that for big amount of players, the situation is opposite about the distribution of 0-1. If there are a lot of players ($k \in \omega(\log(n))$, most of the bits should be 1's.
\paragraph{Critical Coordinates}
Let us denote a coordinate $i$ critical if:
\begin{equation*}
    \Pr[i \in \bigcap^{k}_{j=1}{X_j}] > \frac{\epsilon}{n}
\end{equation*}
\subsection{The Protocol}
Every one of the players are calculating what are the important coordinates.
Every player (one by one) sends the indexes of his zeros which are in important coordinates to the coordinator.
If there is an important coordinate with no zero - the coordinatore declares intersection.
Otherwise, declare - disjointness.
\paragraph{Communication}
We are going to talk to all of the players $k$.
Now we should ask about the expected numbers of zeros in the important coordinates. We are going to pay $\log(n)$ for each one of them.
The expected number of zeros is:
\begin{equation*}
    nk\left(1-\left(\frac{\epsilon}{n}\right)^{1/k}\right) = nk\left(1-e^{\frac{\log{\frac{\epsilon}{n}}}{k}}\right) \sim nk\frac{\log{\frac{n}{\epsilon}}}{k} = n\log{\frac{n}{\epsilon}}
\end{equation*}
So total expected communication complexity for this protocol is
\begin{equation*}
    k + n\log{\frac{n}{\epsilon}}\log n
\end{equation*}
\paragraph{Error}
We error only when declaring disjoitness and there is an intersection in unimportant coordinate.
\begin{equation*}
    \Pr[\text{ERROR}] = \Pr[\text{i unimportant} \land i \in \bigcap_j X_j] \leq \Sigma_i \Pr[\text{i unimportant} \land i \in \bigcap
    _j X_j] \leq n\frac{\epsilon}{n} = \epsilon
\end{equation*}
\begin{claim}
For $p_1, p_2, ... , p_k \in [0,1]$ \newline
If
\begin{equation*}
    \Pi_{i=1}^{k}p_i \geq \frac{\epsilon}{n}
\end{equation*}
We can know that
\begin{equation*}
    \Sigma_{i=1}^{k}p_i \geq k\left(\frac{\epsilon}{n}\right)^{1/k}
\end{equation*}
\end{claim}
\begin{proof}
By lagrange multipliers let us denote a target function 
\begin{equation*}
    f(x_1, ... , x_k) = \Sigma_{i=1}^{k}x_i
\end{equation*}
A constraint function
\begin{equation*}
    g(x_1, ... , x_k) = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n} 
\end{equation*}
Lagrange function
\begin{equation*}
    \mathcal{L}(x_1, ... , x_k, \lambda) = \Sigma_{i=1}^{k}x_i - \lambda\left(\Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}  \right)
\end{equation*}
\begin{align*}
    \frac{\partial\mathcal{L}}{\partial x_i} = 1 - \lambda\Pi_{j \neq i}x_j  \\
    \frac{\partial\mathcal{L}}{\partial \lambda} = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}
\end{align*}
By (2):
\begin{align*}
    \Pi_{i=1}^{k}x_i = \frac{\epsilon}{n} \\
    1 = \frac{\lambda\frac{\epsilon}{n}}{x_i} \\ 
    x_i = \lambda\frac{\epsilon}{n} \\
    \left(\lambda\frac{\epsilon}{n}\right)^k = \frac{\epsilon}{n} \\
    \lambda = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k} - 1} \\
    x_i = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}} \\
    f_{\text{min}} = k\left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}
\end{align*}
\end{proof}
\section{Lower Bound}
\paragraph{Introduction}
Usually in lower bound using information theory techniques, we firstly move to the problem of disjointness where $n=1$ denoted as $\text{AND}_k$ which is the problem where every player has one bit and they need to answer whether they all got 1 or not. After moving to this problem, we calculate how much information is needed in order to solve it.
\subsection{Direct Sum}
\begin{theorem}
Given a protocol $A$ which solves $DISJ^n_k$ with error $\epsilon$ (for every input). There is a protocol $B$ which solves $AND_k$ with error $\epsilon + ??$ such that \newline
For $X \sim \mu^n, \Pi \sim A(X), X' \sim \mu, \Pi ' \sim B(X') $ \newline
$I(\Pi, X) \geq n I(\Pi ', X')$
\end{theorem}
\paragraph{The Protocol}
For input $X'_i \in \{0, 1\}$, the players use the public randomness in order to find a random index $j \in [k]$ in which they are gonna insert the original input. For the other indexes, every player uses the private randomness in order to sample the other indexes from $\mu$. Then they just run the protocol $A$ for their inputs. They answer the output of $A$. 

\paragraph{Error Analysis}
Our protocol errors in one of two cases: The original protocol errors, or the inputs that the player sampled has intersection and the given doesn't. \newline
$\Pr[Error] \leq \epsilon + \Pr[\underset{j \in [k]}{\bigcap } X^j_{-i} \neq \emptyset] = \epsilon + 1 - (1 - \frac{c}{n})^{n-1} \approx \epsilon + 1 - \frac{1}{\sqrt[c]{e}}$ \newline
Here we have a big obstacle since the error is huge (the one coordinate chance to be disjoint is $\frac{1}{n}$ which alone is problematic since it goes smaller). This hints us that we can not use a constant $\epsilon$ for $AND_k$ and our distribution.

\paragraph{Information Analysis}
$X_j - \text{Input for all of the players in coordinate j}$ \newline
$I(X, \Pi) = \underset{j\in[k]}{\Sigma} I[X_j, \Pi | X_{<j}] \overset{(1)}{\geq } \underset{j\in[k]}{\Sigma} I[X_j, \Pi]$ \newline
$I(X', \Pi ') = \underset{j \in [k]}{\mathbb{E}}[I(X_j, \Pi] = \frac{1}{n}\underset{j\in[k]}{\Sigma} I[X_j, \Pi] \leq \frac{1}{n}I(X, \Pi)$ \newline
(1) - Since the coordinates are independent

\subsection{$AND_k$ Information Cost}
\paragraph{Introduction}
We are going to bound the information cost for a protocol which solves $AND_k$. Our analysis is divided into three blocks: \newline
1 - Using the error of the protocol in order to conclude it must know some information about specific player's input. \newline
2 - Distinguishing this input is leaking some information (specifically KL-Divergence) \newline
3 - Concluding this for the general information cost of the protocol \newline
\paragraph{Definitions}
\begin{definition}
$D(a || b) = \underset{x}{\Sigma}a(x)\log(\frac{a(x)}{b(x)})$
\end{definition}
\paragraph{Protocol properties}
For a transcript $\pi$ and player $i$ there is a function $q_i(\pi, x_i) \in [0,1]$ where \newline
$Pr[\pi | x] = \underset{i}{\Pi}q_i(\pi, x_i )$
\begin{definition}
$\lambda _i (\pi) = \frac{q_i(\pi, 0)}{q_i(\pi, 1)}$
\end{definition}
We should think about this as how much this transcript prefers that $x_i = 0$ over $x_i = 1$.
\paragraph{Part 1 - Protocol Error Analysis}
\begin{lemma}
For any $x \in \{0,1\}^k$, any transcript $\pi$ \newline
Denote $Z(x) = {i \in k | x_i = 0}$ so \newline
$\frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \Pi_{i \in Z(x)} \lambda_i (\pi)$
\end{lemma}
\begin{proof}
$\frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \frac{\Pi_{i \in [k]} q_i (\pi, x_i)}{\Pi_{i \in [k]} q_i (\pi, 1)} = \Pi_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)} = \Pi_{i \in Z(x)} \lambda_i (\pi)$
\end{proof}
Let there be $\alpha \in \mathbb{R}, \alpha \geq 1$ \newline
Let us define a set of transcripts $A = \{\pi | \forall_i \lambda_i (\pi) < \alpha \}$. 
Denote two more sets: $T_1$ - transcripts which are ended with positive answer (there is an intersection). $T_0$ - transcripts which are ended with negative answer (there is no intersection). \newline
Let us pay attention: \newline
$\Pr[A \bigcap T_1 | x = 0^k] \leq \Pr[T_1 | x = 0^k] = \Pr[\text{ERROR} | x = 0^k] \leq \epsilon$ \newline
This is since the right answer for $0^k$ is 0 so $T_1$ is error. \newline 
$\Pr[A \bigcap T_0 | x = 0^k] \leq \alpha ^k \Pr[A \bigcap T_0 | x = 1^k] \leq \alpha ^k \Pr[\text{ERROR} | x = 1^k] \leq \alpha ^k \Pr[T_0| x = 1^k] \leq \alpha ^k \epsilon$ \newline
$\Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k) $ \newline
The set in which the transcripts don't really prefer 0 over 1 isn't very common under $x = 0^k$. \newline
We can also see that $\Pr[A^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k) $ \newline
Pay attention that $A^\complement = \{\pi | \exists_i \lambda_i(\pi) \geq \alpha\}$. \newline
If we define $A_{i}^\complement = \{\pi | \lambda_i(\pi) \geq \alpha\}$, we can see that $A^\complement = \bigcup_{i} A_{i}^\complement$.
Using this fact and union bound, there exists $i$ such that 
\begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
\end{equation*}
That is the finish line of this part. We got a set of transcripts which has nice probability under $x = 0^k$ where the transcripts prefer strongly 0 over 1 for some index. We may use this technique in different ways depends on our distribution of inputs. For small $k$ our distribution give high probability for $0^k$ which is pretty convenient. For other distribution we may want to modify this method. \newline
This is a rough method in order to use the fact that the protocol has to be "biased" in terms of $\lambda_i$ in order to have a low error. \newline
\paragraph{Part 2 - Divergence}
In this part, we are going to show that the set we found in the last part is contributing high enough divergence. This will be enough since $x = 0^k$ has high probability under our distribution. \newline
Let us analyze the connection between $\lambda_i(\pi)$ to its divergence. \newline
Pay attention to this: \newline
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] \overset{(1)}{=} \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi|X_{-i} = 0^{k-1}]} = 
\end{equation*}
\begin{equation*}
    = \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}] + \Pr[\pi | X=0^{i-1}10^{k-i}]\Pr[X_i=1|X_{-i}=0^{k-1}]}
\end{equation*}
\begin{equation*}
 =  \frac{\mu(0)}{\mu(0) + \frac{\Pr[\pi | X=0^{i-1}10^{k-i}]}{\Pr[\pi | X=0^k]}\mu(1)} = \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} 
\end{equation*} 
(1) - by Bayes \newline
We can also conclude: \newline
\begin{equation*}
\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]  = \frac{\frac{\mu(1)}{\lambda_i(\pi)}}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
\end{equation*}
The brave part now (analysing two parts of the divergence): \newline
\paragraph{Part 1 of the divergence:}
$\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)} \log\left( \frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) = -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)}$ \newline
It is true that $\frac{logx}{x} \rightarrow 0$ for $x \rightarrow \infty$ \newline
For small k, $\mu(0) \rightarrow 1$. We can choose big enough $\alpha$ such that for big n, if $\lambda_i(\pi) \geq \alpha$ it is true that: \newline
$\lambda_i(\pi)\mu(0) + \mu(1) \geq \lambda_i(\pi)\mu(0) \geq \frac{\lambda_i(\pi)}{2} \geq \frac{\alpha}{2}$ \newline
$ \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)} < \frac{1}{8}$ \newline
To conclude this part:
$\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)} \log\left( \frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) \geq -\mu(1) \frac{1}{8}$
\paragraph{Part 2 of the divergence:}
$\frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\right) \geq \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\alpha}}\right) = \frac{\mu(0)}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\right) = \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(\frac{1}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) = \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(1 + \frac{\mu(1)(1 -\frac{1}{\alpha})}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) \geq \frac{\mu(0)(\mu(1)(1 -\frac{1}{\alpha}))}{2\left(1 - \mu(1)(1 -\frac{1}{\alpha})\right)^2} \geq \frac{\mu(1)}{4}$ \newline
$\log(1+x) \sim x$ for $x \rightarrow 0$
\paragraph{Conclude Divergence part}
If $\lambda_i(\pi) \geq \pi$, for big enough $n$:
$D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \geq \frac{\mu(1)}{4} $
\paragraph{Part 3 - Information Summary}
\begin{equation*}
    I(X;\Pi) = \overset{k+i}{\underset{j=i+1}{\Sigma } } I[X_{j \% k}, \Pi | X_{<(j \% k)}] \geq I(X_i;\Pi | X_{-i}) = \mathbb{E}_{x, \pi \sim \mu} \left( D\left(\frac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right) \geq
\end{equation*}
\begin{equation*}
    \geq \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq 
\end{equation*}
\begin{equation*}
\mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \left (1 - n^{-\frac{1}{k}} \right)^{k} \cdot \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4} \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\section{Extra}
\paragraph{Binary Distribution}
Let us consider a specific interesting distribution for $n = 2^{k-1}$. We think of $X_i \subseteq \{0, 1, ... , n-1\} $ \newline
For $i \in [k-1]$
  \[
    X_i=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| m_{i-1} = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| m_{i-1} = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
and for $i = k$
  \[
    X_k=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
where $m_i$ is the $i$'th bit of m in binary representation. ($m_i \mathrel{\mathop:}= ( m \mathop{\&} 2^{i-1} ) \gg i-1 $). \\
Let's pay attention for some simple properties. First of all $\forall_i |X_i| = \frac{n}{2}$. Moreover, it doesn't matter if we permute the players, for $i < k, |A_i| = 0.5|A_{i-1}|$ and generally for $i < k, |A_i| = 2^{k-1-i}$. $\Pr[DISJ] = 0.5$. The thing is that this distribution has a little entropy ($k$).
\section{Appendix}
\paragraph{Rounds analysis for upper bound}
\begin{claim}
    For $k \geq 2$ and every $n$: $n^{1 - \frac{1}{k}} - (n-\frac{n^{1/k}}{2})^{1-\frac{1}{k}} \geq 0.25$
\end{claim}
\begin{proof}
    Let us define a function $f(x) = x^{1-\frac{1}{k}}$. We want to analyze $f(n) - f(n-\frac{n^{1/k}}{2})$. By Lagrange's theorem, there exists a $c \in [0, \frac{n^{1/k}}{2}]$ such that $f(n) - f(n-\frac{n^{1/k}}{2}) = \frac{n^{1/k}}{2} f'(n-c)$. By calculation: $f'(x) = (1-\frac{1}{k})x^{-1/k}$. Therefore $f(n) -  f(n-n^{1/k}) = \frac{n^{1/k}}{2} (1-\frac{1}{k}) (n-c)^{-1/k} = \frac{(1-\frac{1}{k})}{2} (\frac{n}{n-c})^{1/k} \geq (1-\frac{1}{k})/2 \geq 0.25$ for $k \geq 2$ 
\end{proof}
\begin{claim}
    Let us define the function $f(n)$ - rounds for the protocol starting with $n$ bits with $k$ players. This function is defined by $f(n) = 1 + f(n - \frac{n^{1/k}}{2})$ \newline
    $f(i) = 1$ for $i \leq N^{\frac{1}{k}}$ \newline
    We claim that $f(n) \in O(n^{1-\frac{1}{k}})$
\end{claim}
\begin{proof}
For the definition of $O$ notation, we need to prove that there exists a constant $D$ for which $\forall_{n, k \in \mathbb{N}} : f(n) \leq Dn^{1-\frac{1}{k}}$. \newline
Let us prove it by induction: \newline
a. Induction base: \newline
As always $O$ notation is trivial for the base of the induction as we can choose big enough $D$ such that $f(1) < D$ (for example $D=17f(1)+1337$. \newline
b. the induction step: \newline
By definition we know that \newline
$f(n) = 1 + f(n - \frac{n^{1/k}}{2})$ \newline
In this point we can use the inductive hypothesis \newline
$1 + f(n - \frac{n^{1/k}}{2}) \leq 1 + D(n - \frac{n^{1/k}}{2})^{1-\frac{1}{k}}$ \newline
Using the previous claim $(n - \frac{n^{1/k}}{2})^{1-\frac{1}{k}} \leq n^{1-\frac{1}{k}}-0.25$. \newline
$1 + D(n - \frac{n^{1/k}}{2})^{1-\frac{1}{k}} \leq 1 + D(n^{1-\frac{1}{k}}-0.25)$ \newline
Now we got exactly what we needed. \newline
$1 + D(n^{1-\frac{1}{k}}-0.25) = 1 - 0.25D + D(n^{1-\frac{1}{k}}) $ \newline
Pay attention that if we choose $D \geq 4$ \newline
$1 - 0.25D + D(n^{1-\frac{1}{k}}) \leq Dn^{1-\frac{1}{k}}$ \newline
To summarize, for $D \geq 4$, we proved that \newline
$f(n) \leq Dn^{1-\frac{1}{k}}$. \newline
\end{proof}

\subsection{TODO}
Add an universe set U. and pseudo code. 
Tough (est?) distribution\newline
Make sure to rethink how does this protocol end \newline
Does it help to permutate the players? \newline
Do we have to tell every player everything? maybe we can get rid of the k factor? I need to ask which one of them may have a critical index... \newline
\end{document}
