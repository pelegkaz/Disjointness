\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{paralist}
\usepackage[usenames]{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{authblk}
\usepackage[normalem]{ulem}
\usepackage[ruled,boxed,vlined]{algorithm2e}
\usepackage{float}
\usepackage[colorlinks,urlcolor=black,citecolor=black,linkcolor=black]{hyperref}

\title{Multiparty Disjointness on Product Distributions}
\author{Peleg Kazaz}
\date{August 2019}


%General math style
\newcommand{\fnstyle}[1]{\mathsf{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\coloneq}{:=}
\newcommand{\st}{\medspace \middle| \medspace}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\prob}[1]{\ensuremath{\text{\textsc{#1}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator{\poly}{poly}

%Comments, TODO, etc.
\newcommand{\hide}[1]{ }
\newcommand{\note}[1]{ { \color{blue} #1 } }
\newcommand{\Rnote}[1]{ { \color{magenta} #1 } }
\newcommand{\TODO}[1]{ {\color{red} #1 }}

%Probability
\newcommand{\Ber}{\mathsf{B}}
\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\newcommand{\given}{\medspace \middle| \medspace}
\newcommand{\rv}[1]{\mathbf{#1}}

%Communication & information
\newcommand{\CC}{\mathrm{CC}}
\DeclareMathOperator*{\MI}{I}
\DeclareMathOperator*{\CIC}{CIC}
\DeclareMathOperator{\HH}{H} 


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{invariant}{Invariant}
\newtheorem*{remark}{Remark}

\renewcommand{\include}{\input}

\begin{document}

\maketitle


\section{Introduction}

\subsection{Related Work}
\section{Preliminaries}

\section{$O(kn^{1-1/k})$-Bit Protocol}
\subsection{The Idea}
\Rnote{Not touching this for now}
We are going to consider a sequential point of view for the disjointness problem for k players. Let us imagine a process in which we start with the set of all points ([n]). The players go one after another and intersect their sets with the result. After the last player plays, we end up with the intersection of all sets. Therefore our question is whether this set is empty or not. Moreover, if the last set is empty, one of the players must have subtracted a large amount of elements in this process. We use this fact in order to learn large amount of elements which are not in this player's input. Those elements could not be in the intersection of all of the inputs, so we can ignore them from now on. 
\subsection{Notations}
\Rnote{Not touching this for now}
We use the popular notation: We are going to have $k$ players. For $i \in [k], X_i \in \{0,1\}^{n}$ - the $i$'th player's input. Sometimes we are going to think of $X_i$ as a subset of $[n]$ where $X_i = \{j \in [n] | X_{ij} = 1\}$. In this notation, the distjointness problem is to decide whether $\cap^{k}_{j=1}X_j = \emptyset$. \newline

\subsection{Overview of the Protocol}
Throughout the protocol, we maintain a \emph{universe}, $U \subseteq [n]$,
with the property that $\bigcap_i x_i \subseteq U$.
We initially have $U = [n]$.
The protocol operates in \emph{iterations}, where each iteration has two possible outcomes:
\begin{enumerate}[I.]
  \item We eliminate at least a $1/\Theta(|U|^{1/k})$-fraction of elements from $U$, or
  \item We halt and guess that $\bigcap_i x_i \neq \emptyset$. This guess is correct with high probability over the inputs.
\end{enumerate}
After at most $n^{1 - 1/k}$ iterations, if we have not yet halted,
then $U = \emptyset$, and at this point we announce that $\bigcap_i x_i = \emptyset$.

\subsection{Reducing the Universe Size}
To reduce the size of the universe, we look for a player whose input rules out many elements from being in the intersection,
without communicating a lot.

Define sets $[n] = A_0 \supseteq A_1 \supseteq \ldots \supseteq A_k$, as follows:
for each $i = 1,\ldots,k$,
\begin{equation*}
  A_i \coloneq \cap_{j = 1}^i x_j,
\end{equation*}
so that for each $i \in [k]$ we have $A_i = A_{i-1} \cap x_i$.
Our goal in the $\prob{Disj}$ problem is to determine whether or not $A_k = \emptyset$.

\paragraph{Critical players.}
We claim that if $\bigcap_i x_i = \emptyset$, then there is at least one player $i$ such that $A_i$ is significantly smaller than $A_{i - 1}$;
in other words, $x_i$ eliminates a significant fraction of elements from consideration.
\begin{lemma}
    Let there be a universe $U_r$ (subset of $[n]$). Denote $n_r = |U_r|$. \newline
  If $\bigcap_i x_i = \emptyset$,
  then there is some player $i \in [k]$
  with
  \begin{enumerate}
    \item 
      $|A_{i} \cap U_r| / {|A_{i-1} \cap U_r|} < 1 / n_r^{1/k}$, and,
    \item $A_i \neq \emptyset$.
  \end{enumerate}
  \label{lemma:narrow}
\end{lemma}
\begin{proof}
	We first prove that there is an index $j$ which satisfies the first property, and then we show
	that the minimal such $j$ also has the second property.

  Let $j$ be the largest index such that 
  \begin{equation*}
    A_{j-1} \cap U_r \neq \emptyset.
  \end{equation*}
  There is such a $j$, because $A_0 \cap U_r = U_r \neq \emptyset$.

  We claim that $A_{j} \cap U_r = \emptyset$.
  If $j \neq k$, then this must be true, because $j$ is the maximal index such that $A_{j-1} \neq \emptyset$;
  and if $j = k$, then $A_j = A_k = \bigcap_i x_i = \emptyset$ by assumption.
  Thus,
  \begin{equation*}
    \frac{|A_j \cap U_r|}{|A_{j-1} \cap U_r|} = \frac{0}{|A_{j-1} \cap U_r|} = 0 < \frac{1}{n_r^{1/k}}
  \end{equation*}
  So we know that (1) holds for $j$.
  Let us denote $j^\star$ the minimal index which satisfies (1).
  Because of its minimality, for all $i \in [j^\star-1]$ we have
  \begin{equation*}
    \frac{|A_i \cap U_r|}{|A_{i-1} \cap U_r|} \geq \frac{1}{n_r^{1/k}}.
  \end{equation*}
  Then we get that
  \begin{align*}
    \frac{|A_{j^\star} \cap U_r|}{|A_0 \cap U_r|} = 
    \frac{|A_{j^\star} \cap U_r|}{|A_{j^\star-1} \cap U_r|} \cdot \frac{|A_{j^\star-1} \cap U_r|}{|A_{j^\star-2} \cap U_r|} \cdot \ldots \cdot \frac{|A_1 \cap U_r|}{|A_0 \cap U_r|}
    \geq
    \left(  \frac{1}{n_r^{1/k}} \right)^{j^\star}
    =
    \frac{1}{n_r^{j^\star/k}}.
  \end{align*}
  Since $|A_0 \cap U_r| = n_r$, 
  \begin{align*}
      |A_{j^\star} \cap U_r| \geq n_r^{1 - j^\star/k} \geq 1
  \end{align*}
    Therefore $A_{j^\star} \cap U_r \neq \emptyset$.
\end{proof}

We say that player $i$ is \emph{critical} in $U_r$ if $i$ satisfies the conditions of the lemma.
If player $i$ is critical, then it can efficiently communicate many elements that are not in $A_i$, as shown
by the following simple counting argument:
\begin{lemma}
  Let $Y,Z \subseteq U$ be sets such that $Y \cap Z \neq \emptyset$ and
  \begin{equation*}
    |Y \cap Z| < \frac{|Y|}{n^{1/k}}.
  \end{equation*}
  Let
  \begin{equation*}
    m = \left\lfloor \frac{2|Y|}{n^{1/k}} \right\rfloor.
  \end{equation*}
  Partition the first $m \cdot (n^{1/k}/2)$ elements of $Y$ into sets $Y_1,\ldots,Y_m$,
  each of size $n^{1/k}/2$.
  Then there is some $j \in [m]$ such that $Y_j \cap Z = \emptyset$.
  \label{lemma:reduce}
\end{lemma}  
\begin{proof}
  Suppose for the sake of contradiction that $Y_j \cap Z \neq \emptyset$ for each $j = 1,\ldots,m$.
  Since the $Y_j$'s are disjoint,
  and $Y_1,\ldots,Y_j \subseteq Y$,
  this means that $|Y \cap Z| \geq m$.
  Thus,
  \begin{equation*}
    |Y \cap Z| \geq m = \left\lfloor \frac{2|Y|}{n^{1/k}} \right\rfloor
    \geq
    2\frac{|Y|}{n^{1/k}} - 1.
  \end{equation*}
  By assumption, we have $1 \leq |Y \cap Z| < |Y| / n^{1/k}$.
  This implies that
  \begin{equation*}
    \frac{2|Y|}{n^{1/k}} - 1 > \frac{|Y|}{n^{1/k}}.
  \end{equation*}
  Together, we have that $|Y \cap Z| > |Y| / n^{1/k}$, a contradiction.
\end{proof}
The lemma shows that if $i$ is critical, then by communicating the index $j$ of the set $Z_j$ such that $Z_j \cap x_i = \emptyset$,
we can remove at least $n^{1/k}/2$ elements from consideration.

Note that a player does not know whether it is critical or not, as this depends 
on the other players' inputs;
but each player can compute the \emph{probability} that it is critical, given its input.
For a player $i$,
let
\begin{equation*}
  \gamma_i \coloneq \Pr_{(\rv{X}_1,\ldots,\rv{X}_k) \sim \mu}\left[ \text{ $i$ is critical} \land \bigcap_i \rv{X}_i = \emptyset \given \rv{X}_i = x_i \right].
\end{equation*}
We say that player $i$ is \emph{significant} if $\gamma_i \geq \eps / k$.

\subsection{The Protocol}
For a given product distribution $\mu : \left(\set{0,1}^N\right)^k \rightarrow [0,1]$, we describe a protocol that errs with probability $\epsilon$.
Our protocol uses public randomness and has $\tilde{O}(kN^{1-1/k})$ bits of expected communication.

Initially, $U = \set{1,\ldots,N}$, and $n = N$.
\begin{enumerate}[(1)]
  \item If $|U| \leq N^{1 - 1/k}$, each player $i$ sends $x_i$ to the coordinator, who computes and outputs the answer.
  \item Otherwise, the coordinator asks each player $i$ whether $i$ is significant (i.e., whether $\gamma_i \geq \eps / k$).
  \item If no player is significant, the coordinator outputs ``not disjoint''. Otherwise, let $i$ be the
    first significant player. The coordinator informs player $i$ that it has been selected.
  \item The coordinator and the players use the public randomness to sample \TODO{how many needed?}
    sets $A_{i-1}^{(1)},\ldots$ from the distribution of $\rv{A}_{i-1}$.
  \item Player $i$ finds the index $j$ of the first set $A_{i-1}^{(j)}$ such that
    \begin{equation*}
      |A_{i-1}^{(j)} \cap X_i| \leq \frac{|A_{i-1}^{(j)}|}{n^{1/k}}.
    \end{equation*}
    It sends the index $j$ to the coordinator, who forwards it to the other players.
  \item All participants set $Y = A_{i-1}^{(j)}$, $m = \lfloor 2|Y| / n^{1/k} \rfloor$,
    and let $Y_1,\ldots,Y_m$ be as in Lemma~\ref{lemma:reduce}.
    Player $i$ sets $Z = x_i$, and finds the index $\ell \in \set{1,\ldots,m}$
    such that $Y_j \cap Z = \emptyset$ (the existence of this index is guaranteed by Lemma~\ref{lemma:reduce}).
    Player $i$ sends index $\ell$ to the coordinator, who forwards it to the other players.
  \item All participants set:
    \begin{itemize}
      \item $U \leftarrow U \setminus Z_{\ell}$,
      \item $n \leftarrow |U|$,
      \item $x_i \leftarrow x_i \cap Y$,
      \item \TODO{how are the priors updated.}
    \end{itemize}
\end{enumerate}

%\begin{algorithm}[H]
  %\SetAlgoLined
  %initialization\;
  %\While{not at end of this document}
  %{
    %read current\;
    %\eIf{understand}{go to next section\;current section becomes this one\;}
    %{go back to the beginning of current section\;}
  %}
  %\caption{How to write algorithms}
%\end{algorithm}



%In each iteration $r$, the coordinator asks each of the $k$ players whether or not they are significant.
%If no player is significant, the coordinator halts and outputs ``not disjoint''.
%If there is some significant player, the coordinator chooses the first player $i$ that is significant,
%and sends the index $i$ to all players.
%
%Next, using public randomness, the coordinator and the players sample infinitely many \TODO{(figure out how many really)}
%iid inputs $(X_1, \ldots , X_k)^1, \ldots (X_1, \ldots , X_k)^2,\ldots \sim \mu((\rv{X_1}, \ldots , \rv{X_k}))$.
%Player $i$ finds the first index $j$ such that he is critical in $(X_1, \ldots X_{i-1}, x_i, X_{i+1}, \ldots , X_k)^j$,
%sends $j$ to the coordinator, and then
%
%and uses 
%We use this sample in order to find elements in $[n] \setminus x_i$ as following: Critical property claims that $|A_{i-1}^j \cap x_i| / |A_{i-1}^j| < 1/n_{r-1}^{1/k}$. Now we split $A_{i-1}^j$ to sets of size $\frac{n_{r-1}^{1/k}}{2}$ by order denoted $Z^{r}_1, \ldots , Z^{r}_m$. .By counting argument, one of them doesn't intersect with $A_{i-1}^j \cap x_i$ - let us denote its index by $l$. Player $i$ sends the index $j$ to the coordinator, along with the index $l$ for the right $Z^{r}_l$ to the coordinator.
%The coordinator disseminates $j, l$ to all the other players.
%
%Observe that $Z^{r}_l \subseteq \overline{X_i}$,
%so the elements of $Z^{r}_l$ can now be removed from the universe:
%\begin{equation*}
  %U_r \leftarrow U_{r-1} \setminus (Z^{r}_l).
%\end{equation*}

%If $n_r \leq n^{1-1/k}$, every player sends his input to the coordinator which calculate the disjointness and sends the output to every player.

%\paragraph{Clarifications}
%We strongly use the fact that this is a product distribution where everyone can sample the inputs and parse them and the critical player's input doesn't affect the distribution. \newline
%For specific inputs, one of the player must be critical if there is an intersection (as proved in the lemma) but it is not easy to know which one of them is critical. 
%\TODO{up to here}



\subsection{Properties of Our Protocol}
Our protocol is using random coins and is described in the coordinator model. It uses an expected value of $O(kn^{1-1/k})$ bits of communication between the players.
\subsection{Analysis}
\paragraph{Error Analysis}
Given that the protocol stopped at round $r$, we are going to argue that we error for at most $\epsilon$. Therefore in overall we don't error more than $\epsilon$.
We only error when we stopped because the protocol guessed the output, i.e no player was significant and actually the players' inputs are disjoint. The other ending of the protocol is when everyone sends their inputs to the coordinator which calculates the exact output and there is no possible error. \newline
$\Pr[ERROR | \text{Guessed at round r}] \overset{(1)}{=} \Pr[DISJ | \text{Guessed at round r}] \overset{(2)}{\leq} $\newline
$\sum\limits_{i=1}^k \Pr[\text{$i$ is critical in $U_{r-1}$} \land \text{DISJ}| \text{Guessed at round r}] \overset{(3)}{=} $ \newline
$\sum\limits_{i=1}^k \underset{X_i=x_i}{\mathop{\mathbb{E}}}\Pr[\text{$i$ is critical in $U_{r-1}$} \land \text{DISJ}| \text{Guessed at round r} \land X_i=x_i] \overset{(4)}{\leq} \sum\limits_{i=1}^k \underset{X_i=x_i}{\mathop{\mathbb{E}}}[\frac{\epsilon}{k}] = \sum\limits_{i=1}^k \frac{\epsilon}{k} = \epsilon$ \newline

$\Pr[ERROR] \overset{(5)}{\leq} \underset{\text{r round}}{\mathop{\mathbb{E}}} [\Pr[ERROR | \text{Guessed at L}] ] \leq \epsilon $ \newline
(1) - The protocol errors only when the sets are disjoint (the protocol declares intersection). \newline
(2) - $  \text{DISJ} = \bigcup_{i=1}^{k}\{\text{$i$ is critical in $U_{r-1}$} \land \text{DISJ} \} $ (using lemma 3.1) \newline
(3) - Law of total expectation \newline
(4) - No player was significant \newline
(5) - If the protocol didn't guess, there was no error
\paragraph{Communication Analysis}
We analyze round by round. \newline
For a single round which isn't the last one, firstly we pay at most $k$ bits in order to find a significant player. After that the coordinator sends his identity to the others: $k\log(k)$ bits. Then the protocol sends $j$ and $l$ to all of the players. $l$ is at most $log(n_{r-1})$
Let us analyze how big is $j$ \newline
This is a random variable which has a geometric distribution. \newline
$ \mathop{\mathbb{E}} [J] = \frac{1}{\Pr[\text{$i$ is critical in $U_{r-1}$} \land \text{DISJ} | X_i = x_i]} \leq \frac{k}{\epsilon}$ \newline
$\mathop{\mathbb{E}} [\log(J)] \leq \log(\mathop{\mathbb{E}} [J]) \leq \log(\frac{k}{\epsilon}) = \log(k) + \log(\frac{1}{\epsilon})$ \newline
So in total the cost of the round is $k + k\log(k) + k(\log(k) + \log(\frac{1}{\epsilon})) + k\log(n) \in O(k(\log(n) + \log(\frac{1}{\epsilon}))$ and we discovered $(n_r^{\frac{1}{k}}/2)$ zeros. \newline
In the appendix - there are $O(n^{1-1/k})$ rounds.
For the last round we pay $kn^{1-\frac{1}{k}} \log(n)$ \newline
So the total communication cost is  $O(kn^{1-\frac{1}{k}}(\log(\frac{1}{\epsilon}) + \log(n)))$.
\section{k-n relation analysis}
\paragraph{$k < \log(n)$:}
For this case $k$ is pretty small. meaning $kn^{1/k} \approx n^{1/k}$. So we are pretty tight (our lower bound is almost like our upper one).
\paragraph{$k = \alpha\log(n)$:}
\begin{equation*}
    n^{1/k} = n^{\frac{1}{\alpha\log(n)}} = 2^{\frac{\log(n)}{\alpha\log(n)}} = 2^{1/\alpha}
\end{equation*}
so
\begin{equation*}
    n^{1-1/k} > n/2
\end{equation*}
Moreover, in this scenario:
\begin{equation*}
    p = \frac{1}{n^{1/k}} = \frac{1}{2^{\frac{1}{\alpha}}} 
\end{equation*}
\begin{equation*}
    h(p) = c 
\end{equation*}
\begin{equation*}
    h(\rv{X}) = cnk
\end{equation*}
So our lower bound may also be fine.
\paragraph{$k \in \omega(\log(n))$:}
This is an interesting case. Most of the elements in our input should be 1. 
Let us denote:
\begin{equation*}
    p_{ij} = \Pr[i \in X_j]
\end{equation*}
\begin{equation*}
    \Pr[\neg \text{DISJ}] \leq \Sigma_{i=1}^{n}\Pr[i \in \cap_{j=1}^{k}X_j] = \Sigma_{i=1}^{n}\Pi_{j=1}^{k}p_{ij}
\end{equation*}
Let us look at a specific coordinate $i$. Assume $\Pi_{j=1}^{k}p_{ij} \geq \frac{\epsilon}{n}$.
By lagrange multipliers we can know that $\Sigma_{j=1}^{k}p_{ij} \geq k(\frac{\epsilon}{n})^{\frac{1}{k}}$.
Pay attention that:
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros in i}] = k - \Sigma_{j=1}^{k}p_{ij} \leq k \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right)
\end{equation*}
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) = \left(1 - e^{\frac{\log(\frac{\epsilon}{n})}{k}}\right) = \left(1 - e^{\frac{\log(\epsilon) -\log(n)}{k}}\right)
\end{equation*}
If $k = \log(n) \alpha(n)$
\begin{equation*}
    \left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \left(1 - e^{\frac{1}{\alpha(n)}}\right) \sim \frac{1}{\alpha(n)}
\end{equation*}
We can conclude that
\begin{equation*}
    \mathop{\mathbb{E}}[\text{Zeros}] = nk\left(1 - \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}\right) \sim \frac{nk}{\alpha(n)} = n\log(n)
\end{equation*}
So if our protocol just think about the important indexes (the one which has probability to be in the intersection) and just ask every player to send the indexes of his zeros.
\begin{equation*}
    \text{CC} = k + n\log^2(n)
\end{equation*}
\section{$O(k + n\log^2(n))$-Protocol}
Let us pay attention that for big amount of players, the situation is opposite about the distribution of 0-1. If there are a lot of players ($k \in \omega(\log(n))$, most of the bits should be 1's.
\paragraph{Critical Coordinates}
Let us denote a coordinate $i$ critical if:
\begin{equation*}
    \Pr[i \in \bigcap^{k}_{j=1}{X_j}] > \frac{\epsilon}{n}
\end{equation*}
\subsection{The Protocol}
Every one of the players are calculating what are the important coordinates.
Every player (one by one) sends the indexes of his zeros which are in important coordinates to the coordinator.
If there is an important coordinate with no zero - the coordinatore declares intersection.
Otherwise, declare - disjointness.
\paragraph{Communication}
We are going to talk to all of the players $k$.
Now we should ask about the expected numbers of zeros in the important coordinates. We are going to pay $\log(n)$ for each one of them.
The expected number of zeros is:
\begin{equation*}
    nk\left(1-\left(\frac{\epsilon}{n}\right)^{1/k}\right) = nk\left(1-e^{\frac{\log{\frac{\epsilon}{n}}}{k}}\right) \sim nk\frac{\log{\frac{n}{\epsilon}}}{k} = n\log{\frac{n}{\epsilon}}
\end{equation*}
So total expected communication complexity for this protocol is
\begin{equation*}
    k + n\log{\frac{n}{\epsilon}}\log n
\end{equation*}
\paragraph{Error}
We error only when declaring disjoitness and there is an intersection in unimportant coordinate.
\begin{equation*}
    \Pr[\text{ERROR}] = \Pr[\text{i unimportant} \land i \in \bigcap_j X_j] \leq \Sigma_i \Pr[\text{i unimportant} \land i \in \bigcap
    _j X_j] \leq n\frac{\epsilon}{n} = \epsilon
\end{equation*}
\begin{claim}
For $p_1, p_2, ... , p_k \in [0,1]$ \newline
If
\begin{equation*}
    \Pi_{i=1}^{k}p_i \geq \frac{\epsilon}{n}
\end{equation*}
We can know that
\begin{equation*}
    \Sigma_{i=1}^{k}p_i \geq k\left(\frac{\epsilon}{n}\right)^{1/k}
\end{equation*}
\end{claim}
\begin{proof}
By lagrange multipliers let us denote a target function 
\begin{equation*}
    f(x_1, ... , x_k) = \Sigma_{i=1}^{k}x_i
\end{equation*}
A constraint function
\begin{equation*}
    g(x_1, ... , x_k) = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n} 
\end{equation*}
Lagrange function
\begin{equation*}
    \mathcal{L}(x_1, ... , x_k, \lambda) = \Sigma_{i=1}^{k}x_i - \lambda\left(\Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}  \right)
\end{equation*}
\begin{align*}
    \frac{\partial\mathcal{L}}{\partial x_i} = 1 - \lambda\Pi_{j \neq i}x_j  \\
    \frac{\partial\mathcal{L}}{\partial \lambda} = \Pi_{i=1}^{k}x_i - \frac{\epsilon}{n}
\end{align*}
By (2):
\begin{align*}
    \Pi_{i=1}^{k}x_i = \frac{\epsilon}{n} \\
    1 = \frac{\lambda\frac{\epsilon}{n}}{x_i} \\ 
    x_i = \lambda\frac{\epsilon}{n} \\
    \left(\lambda\frac{\epsilon}{n}\right)^k = \frac{\epsilon}{n} \\
    \lambda = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k} - 1} \\
    x_i = \left(\frac{\epsilon}{n}\right)^{\frac{1}{k}} \\
    f_{\text{min}} = k\left(\frac{\epsilon}{n}\right)^{\frac{1}{k}}
\end{align*}
\end{proof}
\section{Lower Bound}
\paragraph{Introduction}
Usually in lower bound using information theory techniques, we firstly move to the problem of disjointness where $n=1$ denoted as $\text{AND}_k$ which is the problem where every player has one bit and they need to answer whether they all got 1 or not. After moving to this problem, we calculate how much information is needed in order to solve it.
\subsection{Direct Sum}
\begin{theorem}
Given a protocol $A$ which solves $DISJ^n_k$ with error $\epsilon$ (for every input). There is a protocol $B$ which solves $AND_k$ with error $\epsilon + ??$ such that \newline
For $X \sim \mu^n, \Pi \sim A(X), X' \sim \mu, \Pi ' \sim B(X') $ \newline
$I(\Pi, X) \geq n I(\Pi ', X')$
\end{theorem}
\paragraph{The Protocol}
For input $X'_i \in \{0, 1\}$, the players use the public randomness in order to find a random index $j \in [k]$ in which they are gonna insert the original input. For the other indexes, every player uses the private randomness in order to sample the other indexes from $\mu$. Then they just run the protocol $A$ for their inputs. They answer the output of $A$. 

\paragraph{Error Analysis}
Our protocol errors in one of two cases: The original protocol errors, or the inputs that the player sampled has intersection and the given doesn't. \newline
$\Pr[Error] \leq \epsilon + \Pr[\underset{j \in [k]}{\bigcap } X^j_{-i} \neq \emptyset] = \epsilon + 1 - (1 - \frac{c}{n})^{n-1} \approx \epsilon + 1 - \frac{1}{\sqrt[c]{e}}$ \newline
Here we have a big obstacle since the error is huge (the one coordinate chance to be disjoint is $\frac{1}{n}$ which alone is problematic since it goes smaller). This hints us that we can not use a constant $\epsilon$ for $AND_k$ and our distribution.

\paragraph{Information Analysis}
$X_j - \text{Input for all of the players in coordinate j}$ \newline
$I(X, \Pi) = \underset{j\in[k]}{\Sigma} I[X_j, \Pi | X_{<j}] \overset{(1)}{\geq } \underset{j\in[k]}{\Sigma} I[X_j, \Pi]$ \newline
$I(X', \Pi ') = \underset{j \in [k]}{\mathbb{E}}[I(X_j, \Pi] = \frac{1}{n}\underset{j\in[k]}{\Sigma} I[X_j, \Pi] \leq \frac{1}{n}I(X, \Pi)$ \newline
(1) - Since the coordinates are independent

\subsection{$AND_k$ Information Cost}
\paragraph{Introduction}
We are going to bound the information cost for a protocol which solves $AND_k$. Our analysis is divided into three blocks: \newline
1 - Using the error of the protocol in order to conclude it must know some information about specific player's input. \newline
2 - Distinguishing this input is leaking some information (specifically KL-Divergence) \newline
3 - Concluding this for the general information cost of the protocol \newline
\paragraph{Definitions}
\begin{definition}
$D(a || b) = \underset{x}{\Sigma}a(x)\log(\frac{a(x)}{b(x)})$
\end{definition}
\paragraph{Protocol properties}
For a transcript $\pi$ and player $i$ there is a function $q_i(\pi, x_i) \in [0,1]$ where \newline
$Pr[\pi | x] = \underset{i}{\Pi}q_i(\pi, x_i )$
\begin{definition}
$\lambda _i (\pi) = \frac{q_i(\pi, 0)}{q_i(\pi, 1)}$
\end{definition}
We should think about this as how much this transcript prefers that $x_i = 0$ over $x_i = 1$.
\paragraph{Part 1 - Protocol Error Analysis}
\begin{lemma}
For any $x \in \{0,1\}^k$, any transcript $\pi$ \newline
Denote $Z(x) = {i \in k | x_i = 0}$ so \newline
$\frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \Pi_{i \in Z(x)} \lambda_i (\pi)$
\end{lemma}
\begin{proof}
$\frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \frac{\Pi_{i \in [k]} q_i (\pi, x_i)}{\Pi_{i \in [k]} q_i (\pi, 1)} = \Pi_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)} = \Pi_{i \in Z(x)} \lambda_i (\pi)$
\end{proof}
Let there be $\alpha \in \mathbb{R}, \alpha \geq 1$ \newline
Let us define a set of transcripts $A = \{\pi | \forall_i \lambda_i (\pi) < \alpha \}$. 
Denote two more sets: $T_1$ - transcripts which are ended with positive answer (there is an intersection). $T_0$ - transcripts which are ended with negative answer (there is no intersection). \newline
Let us pay attention: \newline
$\Pr[A \bigcap T_1 | x = 0^k] \leq \Pr[T_1 | x = 0^k] = \Pr[\text{ERROR} | x = 0^k] \leq \epsilon$ \newline
This is since the right answer for $0^k$ is 0 so $T_1$ is error. \newline 
$\Pr[A \bigcap T_0 | x = 0^k] \leq \alpha ^k \Pr[A \bigcap T_0 | x = 1^k] \leq \alpha ^k \Pr[\text{ERROR} | x = 1^k] \leq \alpha ^k \Pr[T_0| x = 1^k] \leq \alpha ^k \epsilon$ \newline
$\Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k) $ \newline
The set in which the transcripts don't really prefer 0 over 1 isn't very common under $x = 0^k$. \newline
We can also see that $\Pr[A^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k) $ \newline
Pay attention that $A^\complement = \{\pi | \exists_i \lambda_i(\pi) \geq \alpha\}$. \newline
If we define $A_{i}^\complement = \{\pi | \lambda_i(\pi) \geq \alpha\}$, we can see that $A^\complement = \bigcup_{i} A_{i}^\complement$.
Using this fact and union bound, there exists $i$ such that 
\begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
\end{equation*}
That is the finish line of this part. We got a set of transcripts which has nice probability under $x = 0^k$ where the transcripts prefer strongly 0 over 1 for some index. We may use this technique in different ways depends on our distribution of inputs. For small $k$ our distribution give high probability for $0^k$ which is pretty convenient. For other distribution we may want to modify this method. \newline
This is a rough method in order to use the fact that the protocol has to be "biased" in terms of $\lambda_i$ in order to have a low error. \newline
\paragraph{Part 2 - Divergence}
In this part, we are going to show that the set we found in the last part is contributing high enough divergence. This will be enough since $x = 0^k$ has high probability under our distribution. \newline
Let us analyze the connection between $\lambda_i(\pi)$ to its divergence. \newline
Pay attention to this: \newline
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] \overset{(1)}{=} \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi|X_{-i} = 0^{k-1}]} = 
\end{equation*}
\begin{equation*}
    = \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}] + \Pr[\pi | X=0^{i-1}10^{k-i}]\Pr[X_i=1|X_{-i}=0^{k-1}]}
\end{equation*}
\begin{equation*}
 =  \frac{\mu(0)}{\mu(0) + \frac{\Pr[\pi | X=0^{i-1}10^{k-i}]}{\Pr[\pi | X=0^k]}\mu(1)} = \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} 
\end{equation*} 
(1) - by Bayes \newline
We can also conclude: \newline
\begin{equation*}
\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]  = \frac{\frac{\mu(1)}{\lambda_i(\pi)}}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
\end{equation*}
The brave part now (analysing two parts of the divergence): \newline
\paragraph{Part 1 of the divergence:}
$\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)} \log\left( \frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) = -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)}$ \newline
It is true that $\frac{logx}{x} \rightarrow 0$ for $x \rightarrow \infty$ \newline
For small k, $\mu(0) \rightarrow 1$. We can choose big enough $\alpha$ such that for big n, if $\lambda_i(\pi) \geq \alpha$ it is true that: \newline
$\lambda_i(\pi)\mu(0) + \mu(1) \geq \lambda_i(\pi)\mu(0) \geq \frac{\lambda_i(\pi)}{2} \geq \frac{\alpha}{2}$ \newline
$ \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)} < \frac{1}{8}$ \newline
To conclude this part:
$\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)} \log\left( \frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) \geq -\mu(1) \frac{1}{8}$
\paragraph{Part 2 of the divergence:}
$\frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\right) \geq \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\alpha}}\right) = \frac{\mu(0)}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\right) = \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(\frac{1}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) = \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(1 + \frac{\mu(1)(1 -\frac{1}{\alpha})}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) \geq \frac{\mu(0)(\mu(1)(1 -\frac{1}{\alpha}))}{2\left(1 - \mu(1)(1 -\frac{1}{\alpha})\right)^2} \geq \frac{\mu(1)}{4}$ \newline
$\log(1+x) \sim x$ for $x \rightarrow 0$
\paragraph{Conclude Divergence part}
If $\lambda_i(\pi) \geq \pi$, for big enough $n$:
$D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \geq \frac{\mu(1)}{4} $
\paragraph{Part 3 - Information Summary}
\begin{equation*}
    I(X;\Pi) = \overset{k+i}{\underset{j=i+1}{\Sigma } } I[X_{j \% k}, \Pi | X_{<(j \% k)}] \geq I(X_i;\Pi | X_{-i}) = \mathbb{E}_{x, \pi \sim \mu} \left( D\left(\frac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right) \geq
\end{equation*}
\begin{equation*}
    \geq \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq 
\end{equation*}
\begin{equation*}
\mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \left (1 - n^{-\frac{1}{k}} \right)^{k} \cdot \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4} \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\section{Extra}
\paragraph{Binary Distribution}
Let us consider a specific interesting distribution for $n = 2^{k-1}$. We think of $X_i \subseteq \{0, 1, ... , n-1\} $ \newline
For $i \in [k-1]$
  \[
    X_i=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| m_{i-1} = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| m_{i-1} = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
and for $i = k$
  \[
    X_k=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
where $m_i$ is the $i$'th bit of m in binary representation. ($m_i \mathrel{\mathop:}= ( m \mathop{\&} 2^{i-1} ) \gg i-1 $). \\
Let's pay attention for some simple properties. First of all $\forall_i |X_i| = \frac{n}{2}$. Moreover, it doesn't matter if we permute the players, for $i < k, |A_i| = 0.5|A_{i-1}|$ and generally for $i < k, |A_i| = 2^{k-1-i}$. $\Pr[DISJ] = 0.5$. The thing is that this distribution has a little entropy ($k$).
\section{Appendix}
\paragraph{Rounds analysis for upper bound}
\begin{claim}
    Let us define the function $f(n)$ - rounds for the protocol starting with $n$ bits.
    $f(n) = 1 + f(n - n^{\frac{1}{k}}/2)$ \newline
    We claim that $f(n) \in n^{1-\frac{1}{k}}$
\end{claim}
\begin{proof}
We should find that $ \exists_{D\in\mathbb{R}} : \forall_{n \in \mathbb{N}} : f(n) \leq Dn^{1-\frac{1}{k}}$. Let us prove by induction. The induction base is trivial (we can choose big enough $D$). The induction step: \newline
We can think of $\epsilon$ as a constant. \newline
$f(n) = 1 + f(n - n^{\frac{1}{k}}/2) \leq 1 + D(n - n^{\frac{1}{k}}/2)^{1-\frac{1}{k}} \overset{?}{\leq} Dn^{1-\frac{1}{k}}$ \newline

\begin{align*}
  1 &\overset{?}{\leq} Dn^{1-\frac{1}{k}} -  D(n - n^{\frac{1}{k}}/2)^{1-\frac{1}{k}}\\
  1 &\overset{?}{\leq} D(n^{1-\frac{1}{k}} -  n^{\frac{1}{k}}/2)^{1-\frac{1}{k}})\\
  \frac{1}{n^{1-\frac{1}{k}} -  (n - n^{\frac{1}{k}}/2)^{1-\frac{1}{k}}} & \overset{?}{\leq} D \\
  \frac{n^{-1+\frac{1}{k}}}{1 -  (1 - \frac{1}{2n^{1 - \frac{1}{k}}})^{1-\frac{1}{k}}} & \overset{?}{\leq} D \\  
\end{align*}
$\lim_{n \rightarrow \infty} \frac{n^{1-\frac{1}{k}}}{1 -  (1 - \frac{1}{2n^{1 - \frac{1}{k}}})^{1-\frac{1}{k}}} \overset{(L)}{=} $
\newline
(L) - L'Hôpital's rule
\end{proof}
\paragraph{Proof for $f(n) \in O(kn^{1-1/k})$}
\begin{claim} 
For $f(n) = k(\log(k) + \log(\frac{1}{\epsilon}) + m) + f(n - m(n^{1/k} - 1))$, it is true that $f(n) \in O(kn^{1-1/k})$ (when thinking of $\epsilon$ as a constant). \newline
\end{claim}
\begin{proof}
We should find that $ \exists_{D\in\mathbb{R}} : \forall_{n \in \mathbb{N}} : f(n) \leq Dkn^{1-1/k}$. Let us prove by induction. The induction base is trivial (we can choose big enough $D$). The induction step: \newline
We can think of $\epsilon$ as a constant. \newline
$f(n) = k(\log(k) + m) + f(n - m(n^{1/k} - 1)) \leq k(\log(k) + m) + Dk(n - m(n^{1/k} - 1))^{1-1/k} \overset{?}{\leq} Dkn^{1-1/k}$ \newline

\begin{align*}
  k(\log(k) + m) &\overset{?}{\leq} Dkn^{1-1/k} -  Dk(n - m(n^{1/k} - 1))^{1-1/k}\\
  (\log(k) + m) &\overset{?}{\leq} D(n^{1-1/k} -  (n - m(n^{1/k} - 1))^{1-1/k})\\
  \frac{\log(k) + m}{n^{1-1/k} -  (n - m(n^{1/k} - 1))^{1-1/k}} & \overset{?}{\leq} D \\
\end{align*}
\newline
Maybe we can just say that the worse case is when $m = 1$.
In this case we have $n^{1-\frac{1}{k}}$ rounds where each one costs $k\log(k)$ so $kn^{1-\frac{1}{k}}\log(k)$
\end{proof}

\begin{claim}
\begin{equation*}
    \lim_{n \rightarrow \infty} n^{1-\frac{1}{k}} - \left( n - n^{\frac{1}{k}} \right)^{1-\frac{1}{k}} = 1-\frac{1}{k}
\end{equation*}
\end{claim}
\begin{proof}
$$\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}} - \left( n - n^{\frac{1}{k}} \right)^{1-\frac{1}{k}} = 
\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}} \left(1 - \frac{\left( n - n^{\frac{1}{k}} \right)^{1-\frac{1}{k}}}{n^{1-\frac{1}{k}}}\right) = 
\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}}\left(1 - \left( \frac{n - n^{\frac{1}{k}} }{n}\right)^{1-\frac{1}{k}}\right) = $$ \newline
$$
\lim_{n \rightarrow \infty} n^{1-\frac{1}{k}}\left(1 - \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{1-\frac{1}{k}}\right) =
\frac{1}{\lim_{n \rightarrow \infty} \frac{n^{-1+\frac{1}{k}}}{1 - \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{1-\frac{1}{k}}}} \overset{(L)}{=} $$ \newline
$$\frac{1}{\lim_{n \rightarrow \infty} \frac{\left(-1+\frac{1}{k}\right)n^{-2+\frac{1}{k}}}{-\left(1-\frac{1}{k}\right) \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{-\frac{1}{k}} \cdot (-1) \cdot \left(-1+\frac{1}{k}\right)n^{-2+\frac{1}{k}}}} =
\frac{1}{\lim_{n \rightarrow \infty} \frac{1}{\left(1-\frac{1}{k}\right) \left( 1 - \frac{1 }{n^{1-\frac{1}{k}}}\right)^{-\frac{1}{k}}}} =$$ \newline
$$
\frac{1}{\frac{1}{\left(1-\frac{1}{k}\right) \left( 1 - 0\right)^{-\frac{1}{k}}}} = \frac{1}{\frac{1}{\left(1-\frac{1}{k}\right)}} = 1-\frac{1}{k}. 
$$

(L) - L'Hôpital's rule
\end{proof}

\subsection{TODO}
Add an universe set U. and pseudo code. 
Tough (est?) distribution\newline
Make sure to rethink how does this protocol end \newline
Does it help to permutate the players? \newline
Do we have to tell every player everything? maybe we can get rid of the k factor? I need to ask which one of them may have a critical index... \newline
\end{document}
