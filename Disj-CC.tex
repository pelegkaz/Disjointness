\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{times}
\usepackage{paralist}
\usepackage[usenames]{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{placeins}
\usepackage{authblk}
\usepackage[normalem]{ulem}
\usepackage[ruled,boxed,vlined]{algorithm2e}
\usepackage{float}
\usepackage[colorlinks,urlcolor=black,citecolor=black,linkcolor=black]{hyperref}

\title{Multiparty Disjointness on Product Distributions}
\author{Peleg Kazaz}
\date{June 2020}


%General math style
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\fnstyle}[1]{\mathsf{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\coloneq}{:=}
\newcommand{\st}{\medspace \middle| \medspace}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\prob}[1]{\ensuremath{\text{\textsc{#1}}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator{\poly}{poly}

\newcommand{\Dfrac}[2]{%
  \ooalign{%
    $\genfrac{}{}{1.2pt}1{#1}{#2}$\cr%
    $\color{white}\genfrac{}{}{.4pt}1{\phantom{#1}}{\phantom{#2}}$}%
}

%Comments, TODO, etc.
\newcommand{\hide}[1]{ }
\newcommand{\note}[1]{ { \color{blue} #1 } }
\newcommand{\Rnote}[1]{ { \color{magenta} #1 } }
\newcommand{\TODO}[1]{ {\color{red} #1 }}

%Probability
\newcommand{\Ber}{\mathsf{B}}
\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\newcommand{\given}{\medspace \middle| \medspace}
\newcommand{\rv}[1]{\mathbf{#1}}

%Communication & information
\newcommand{\CC}{\mathrm{CC}}
\DeclareMathOperator*{\MI}{I}
\DeclareMathOperator*{\CIC}{CIC}
\DeclareMathOperator{\HH}{H} 


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{invariant}{Invariant}
\newtheorem*{remark}{Remark}

\renewcommand{\include}{\input}

\begin{document}

\maketitle

%\begin{algorithm}[H]
  %\SetAlgoLined
  %initialization\;
  %\While{not at end of this document}
  %{
    %read current\;
    %\eIf{understand}{go to next section\;current section becomes this one\;}
    %{go back to the beginning of current section\;}
  %}
  %\caption{How to write algorithms}
%\end{algorithm}

%In each iteration $r$, the coordinator asks each of the $k$ players whether or not they are significant.
%If no player is significant, the coordinator halts and outputs ``not disjoint''.
%If there is some significant player, the coordinator chooses the first player $i$ that is significant,
%and sends the index $i$ to all players.
%
%Next, using public randomness, the coordinator and the players sample infinitely many \TODO{(figure out how many really)}
%iid inputs $(X_1, \ldots , X_k)^1, \ldots (X_1, \ldots , X_k)^2,\ldots \sim \mu((\rv{X_1}, \ldots , \rv{X_k}))$.
%Player $i$ finds the first index $j$ such that he is critical in $(X_1, \ldots X_{i-1}, x_i, X_{i+1}, \ldots , X_k)^j$,
%sends $j$ to the coordinator, and then
%
%and uses 
%We use this sample in order to find elements in $[n] \setminus x_i$ as following: Critical property claims that $|A_{i-1}^j \cap x_i| / |A_{i-1}^j| < 1/n_{r-1}^{1/k}$. Now we split $A_{i-1}^j$ to sets of size $\frac{n_{r-1}^{1/k}}{2}$ by order denoted $Z^{r}_1, \ldots , Z^{r}_m$. .By counting argument, one of them doesn't intersect with $A_{i-1}^j \cap x_i$ - let us denote its index by $l$. Player $i$ sends the index $j$ to the coordinator, along with the index $l$ for the right $Z^{r}_l$ to the coordinator.
%The coordinator disseminates $j, l$ to all the other players.
%
%Observe that $Z^{r}_l \subseteq \overline{X_i}$,
%so the elements of $Z^{r}_l$ can now be removed from the universe:
%\begin{equation*}
  %U \leftarrow U_{r-1} \setminus (Z^{r}_l).
%\end{equation*}

%If $n_r \leq n^{1-\frac{1}{k}}$, every player sends his input to the coordinator which calculates the disjointness and sends the output to every player.

%\paragraph{Clarifications}
%We strongly use the fact that this is a product distribution where everyone can sample the inputs and parse them and the critical player's input doesn't affect the distribution. \newline
%For specific inputs, one of the players must be critical if there is an intersection (as proved in the lemma) but it is not easy to know which one of them is critical. 
%\TODO{up to here}


\section{k-n relation analysis}
Let us pay attention that the mentioned upper bound may be not significant in some cases. Let us pay attention to the case in which $k = \alpha\log(n)$.
\begin{equation*}
    n^{1/k} = n^{\frac{1}{\alpha\log(n)}} = 2^{\frac{\log(n)}{\alpha\log(n)}} = 2^{1/\alpha}
\end{equation*}
In this case
\begin{equation*}
    n^{1-\frac{1}{k}} > n/2 \in \Omega(n)
\end{equation*}
It may seems that our protocol is not very helpful in this case. \newline
Let us consider even bigger number of players: $k \in \omega(\log(n))$. \newline
Now things act differently - For hard distribution most of the elements in the players' input should be 1. 
\section{$O(k + n\log^{2}(n))$-Protocol}
Let us present a protocol which is useful only where we have many players ($k \in \omega(\log(n))$. In this case, most of the bits in our input should be 1's. For simplicity, let us denote
\begin{equation*}
    k = \log(n) \alpha(n)
\end{equation*}

\section{Lower Bound}
\paragraph{Introduction}
Usually in lower bound using information theory techniques, we firstly move to the problem of disjointness where $n=1$ denoted as $\text{AND}_k$ which is the problem where every player has one bit and they need to answer whether they all got 1 or not. After moving to this problem, we calculate how much information is needed in order to solve it.
\subsection{$AND_k$ Information Cost}
\paragraph{Introduction}
We are going to bound the information cost for a protocol that solves $AND_k$. Our analysis is divided into three blocks: \newline
1 - Using the error of the protocol in order to conclude it must know some information about specific player's input. \newline
2 - Distinguishing this input is leaking some information (specifically KL-Divergence) \newline
3 - Concluding this for the general information cost of the protocol \newline
\paragraph{Definitions}

\begin{definition}
KL-Divergence for $D_{KL}(\Dfrac{a}{b})$ is defined by
\begin{equation*} 
    D_{KL}(\Dfrac{a}{b}) = \sum_{x \in \Omega} a(x)\log \left(\frac{a(x)}{b(x)}\right)
\end{equation*}

\begin{lemma}
    For $X, Y, Z$ random variables
    \begin{equation*}
        I(X;Y|Z) = \mathbb{E}_{Y,Z}[D_{KL}\left(\Dfrac{X|Y,Z}{X|Z}\right)]
    \end{equation*}
\end{lemma}

\end{definition}

\begin{definition}
    Denote two sets of transcripts: \newline
    $T_1$ - transcripts which are ended with positive answers (there is an intersection). \newline
    $T_0$ - transcripts which are ended with negative answers (there is no intersection).
\end{definition}

\paragraph{Protocol properties}
For a transcript $\pi$ and player $i$ there is a function $q_i(\pi, x_i) \in [0,1]$ where
\begin{equation*}
    Pr[\pi | x] = \prod_{i=1}^{k}q_i(\pi, x_i )
\end{equation*}

\begin{definition}
\begin{equation*}
    \lambda _i (\pi) = \frac{q_i(\pi, 0)}{q_i(\pi, 1)}
\end{equation*}
We should think about this as how much this transcript prefers that $x_i = 0$ over $x_i = 1$.
\end{definition}

\begin{definition}
For $\alpha \in \mathbb{R}$ where $\alpha \geq 1$, let us define a set of transcripts 
\begin{equation*}
    A = \{\pi | \forall_{i \in [k]} \lambda_i (\pi) < \alpha \}
\end{equation*}
\end{definition}


\paragraph{Part 1 - Protocol Error Analysis}
\begin{lemma}
For any input $x \in \{0,1\}^k$, and any transcript $\pi$. For $Z(x) = \{i \in k | x_i = 0\}$, it is true that  \newline
\begin{equation*}
    \frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \prod_{i \in Z(x)} \lambda_i(\pi)
\end{equation*}
\end{lemma}
\begin{proof}
By definition of $q$ function
\begin{equation*}
    \frac{Pr[\pi | X = x]}{Pr[\pi | X = 1^k]} = \frac{\prod_{i \in [k]} q_i (\pi, x_i)}{\prod_{i \in [k]} q_i (\pi, 1)}
\end{equation*}
By definition of $Z(x)$
\begin{equation*}
    \frac{\prod_{i \in [k]} q_i (\pi, x_i)}{\prod_{i \in [k]} q_i (\pi, 1)} = \prod_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)}
\end{equation*}
By definition of $\lambda_i (\pi)$
\begin{equation*}
    \prod_{i \in Z(x)} \frac{q_i (\pi, 0)}{q_i (\pi, 1)} = \prod_{i \in Z(x)} \lambda_i (\pi)
\end{equation*}
\end{proof}

\begin{lemma}
    \begin{equation*}
        \Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k)
    \end{equation*}
    The set in which the transcripts do not prefer 0 over 1 is not very common under $x = 0^k$.
\end{lemma}
\begin{proof}
For $T_0, T_1$ defined as mentioned, let us bound the probability for $A \bigcap T_0$ and $A \bigcap T_1$. \newline

1. For $A \bigcap T_0$: \newline
Let there be $\pi \in A \bigcap T_0$. \newline
For this $\pi$, let us use the previous lemma for $x = 0^k$
\begin{equation*}
    \frac{Pr[\pi | X = 0^k]}{Pr[\pi | X = 1^k]} = \prod_{i \in Z(0^k)} \lambda_i(\pi)
\end{equation*}
Since $Z(0^k) = [k]$
\begin{equation*}
    \frac{Pr[\pi | X = 0^k]}{Pr[\pi | X = 1^k]} = \prod_{i \in [k]} \lambda_i(\pi)
\end{equation*}
Since $\pi \in A$, we know that $\forall_{i \in [k]}\lambda_i(\pi) < \alpha$
\begin{equation*}
    \frac{Pr[\pi | X = 0^k]}{Pr[\pi | X = 1^k]} < \alpha^{k}
\end{equation*}
Summing over all $\pi \in A \bigcap T_0 $
\begin{equation*}
    \Pr[A \bigcap T_0 | x = 0^k] < \alpha ^k \Pr[A \bigcap T_0 | x = 1^k]
\end{equation*}
Since $A \bigcap T_0 \subseteq T_0$
\begin{equation*}
    \alpha ^k \Pr[A \bigcap T_0 | x = 1^k] \leq \alpha ^k \Pr[T_0 | x = 1^k]
\end{equation*}
Since the answer of disjointness is 1 for the input $x=1^k$, we know that $\{T_0 | x=1^k\} = \{\text{ERROR} | x=1^k\}$ .
\begin{equation*}
    \alpha ^k \Pr[T_0 | x = 1^k] = \alpha ^k \Pr[\text{ERROR} | x = 1^k]
\end{equation*}
For a protocol which errors $\epsilon$ for any input
\begin{equation*}
    \Pr[\text{ERROR} | x = 1^k] \leq \epsilon
\end{equation*}
We now have that 
\begin{equation*}
    \Pr[A \bigcap T_0 | x = 0^k] \leq \alpha^k \epsilon
\end{equation*}
2. For $A \bigcap T_1$: \newline
Since $A \bigcap T_1 \subseteq T_1$
\begin{equation*}
    \Pr[A \bigcap T_1 | x = 0^k] \leq \Pr[T_1 | x = 0^k]
\end{equation*}
Since the answer of disjointness is 0 for the input $x=0^k$, we know that $\{T_1 | x=0^k\} = \{\text{ERROR} | x=0^k\}$ .
\begin{equation*}
    \Pr[T_1 | x = 0^k] = \Pr[\text{ERROR} | x = 0^k]
\end{equation*}
For a protocol which errors $\epsilon$ for any input
\begin{equation*}
    \Pr[\text{ERROR} | x = 0^k] \leq \epsilon
\end{equation*}
We now have that 
\begin{equation*}
    \Pr[A \bigcap T_1 | x = 0^k] \leq \epsilon
\end{equation*}

3. To conclude: \newline
\begin{equation*}
    \Pr[A| x = 0^k] = \leq \Pr[A \bigcap T_0 | x = 0^k] + \Pr[A \bigcap T_1 | x = 0^k] < \epsilon (1 + \alpha ^k)
\end{equation*}
\end{proof}

\begin{lemma}
    For $A_{i}^\complement = \{\pi | \lambda_i(\pi) \geq \alpha\}$, there exists $i$ where
    \begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
    \end{equation*}
\end{lemma}
\begin{proof}
Using the previous lemma,
\begin{equation*}
    \Pr[A| x = 0^k] \leq \epsilon (1 + \alpha ^k)
\end{equation*}
Using set compliment
\begin{equation*}
    \Pr[A^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k)
\end{equation*}
Since $A^\complement = \{\pi | \exists_i \lambda_i(\pi) \geq \alpha\}$, we know that 
\begin{equation*}
    A^\complement = \bigcup_{i} A_{i}^\complement
\end{equation*}
Therefore 
\begin{equation*}
     \sum_{i=1}^{k} \Pr[A_i^\complement| x = 0^k] \geq \Pr[A^\complement| x = 0^k]
\end{equation*}
Combining these facts
\begin{equation*}
    \sum_{i=1}^{k} \Pr[A_i^\complement| x = 0^k] \geq 1 - \epsilon (1 + \alpha ^k)
\end{equation*}
Therefore there exists $i$ where 
\begin{equation*}
    \Pr[A_{i}^\complement| x = 0^k] \geq \frac{1 - \epsilon (1 + \alpha ^k)}{k}
    \end{equation*}
\end{proof}

That is the finish line of this part. We got a set of transcripts that has a nice probability under $x = 0^k$ where the transcripts prefer strongly 0 over 1 for some index. We may use this technique in different ways depends on our distribution of inputs. For small $k$ our distribution gives a high probability for $0^k$ which is pretty convenient. For other distribution, we want to use the other method. \newline
This is a rough method in order to use the fact that the protocol has to be "biased" in terms of $\lambda_i$ in order to have a low error. \newline
\paragraph{Part 2 - Divergence}
In this part, we are going to show that the set we found in the last part is contributing large enough divergence. This will be enough since $x = 0^k$ has a high probability under our distribution. \newline
Let us analyze the connection between $\lambda_i(\pi)$ to its divergence. \newline
\begin{lemma}
    For 0-1 distribution $\mu$, player $i$ and transcript $\pi$
    \begin{equation*}
        \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] = \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}
    \end{equation*}
    \begin{equation*}
        \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi] = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
    \end{equation*}
\end{lemma}
\begin{proof}
    By Bayes  theorem
    \begin{equation*}
        \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] = \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi|X_{-i} = 0^{k-1}]} = 
    \end{equation*}
    By the law of total probability for $\pi|X_{-i} = 0^{k-1}$ with $X_i=0,1$
    \begin{equation*}
        \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{Pr[\pi,X_i=0|X_{-i} = 0^{k-1}] + Pr[\pi,X_i=1|X_{-i} = 0^{k-1}]} = 
    \end{equation*}
    Using conditional probability for the denominator
    \begin{equation*}
        \frac{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}]}{\Pr[\pi | X=0^k]\Pr[X_i=0|X_{-i}=0^{k-1}] + \Pr[\pi | X=0^{i-1}10^{k-i}]\Pr[X_i=1|X_{-i}=0^{k-1}]} =
    \end{equation*}
    Since the input is drawn using product distribution
    \begin{equation*}
        \frac{\Pr[\pi | X=0^k]\mu(0)}{\Pr[\pi | X=0^k]\mu(0) + \Pr[\pi | X=0^{i-1}10^{k-i}]\mu(1)} = 
    \end{equation*}
    Divide both with $\Pr[\pi | X=0^k]$
    \begin{equation*}
        \frac{\mu(0)}{\mu(0) + \frac{\Pr[\pi | X=0^{i-1}10^{k-i}]}{\Pr[\pi | X=0^k]}\mu(1)} = 
    \end{equation*}
    Using the definition of $\lambda_i(\pi)$
    \begin{equation*}
        \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}
    \end{equation*}
    This proves the first equation.
    For the second equation
    \begin{equation*}
        \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi] = 1 - \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]
    \end{equation*}
    Using the first equation
    \begin{equation*}
        1 - \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi] = 1 - \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}
    \end{equation*}
    Some algebraic expansion
    \begin{equation*}
        1 - \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\frac{\mu(1)}{\lambda_i(\pi)}}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}} = \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}
    \end{equation*}
\end{proof}
\begin{lemma}
    For a specific transcript $\pi$ and player $i$ where $\lambda_i(\pi) \geq \alpha$, for big enough $n$
    \begin{equation*}
        D_{KL}\left(\Dfrac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i} = 0^{k-1}} \right) \geq \frac{\mu(1)}{4}
    \end{equation*}
\end{lemma}
\begin{proof}
By definition of KL-Divergence
\begin{equation*}
    D_{KL}\left(\Dfrac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i} = 0^{k-1}} \right) = 
\end{equation*}
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 0 | X_{-i}=0^{k-1}]}\right) +
\end{equation*}
\begin{equation*}
    \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 1 | X_{-i}=0^{k-1}]}\right)
\end{equation*}
\paragraph{Part 1 of the divergence:}
Since the input is drawn using product distribution
\begin{equation*}
    \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 0 | X_{-i}=0^{k-1}]}\right) = \Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 0 | X_{-i}=0^{k-1}, \pi]}{\mu(0)}\right) = 
\end{equation*}
Using the previous lemma
\begin{equation*}
    \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{\frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}}{\mu(0)}\right) = 
\end{equation*}
Some algebraic expansion
\begin{equation*}
    \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\lambda_i(\pi)}}\right) \geq
\end{equation*}
Using $\lambda_i(\pi) \geq \alpha$
\begin{equation*}
    \frac{\mu(0)}{\mu(0) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{\mu(0) + \frac{\mu(1)}{\alpha}}\right) =
\end{equation*}
Since $\mu(0) = 1 - \mu(1)$
\begin{equation*}
    \frac{\mu(0)}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\log\left(\frac{1}{1 - \mu(1) + \frac{\mu(1)}{\alpha}}\right) = 
\end{equation*}
Some algebraic expansion
\begin{equation*}
    \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(\frac{1}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) =
\end{equation*}
Some more algebraic expansion
\begin{equation*}
    \frac{\mu(0)}{1 - \mu(1)(1 -\frac{1}{\alpha})}\log\left(1 + \frac{\mu(1)(1 -\frac{1}{\alpha})}{1 - \mu(1)(1 -\frac{1}{\alpha})}\right) \geq
\end{equation*}
Using $\log(1+x) \sim x$ for $x \rightarrow 0$
\begin{equation*}
    \frac{\mu(0)(\mu(1)(1 -\frac{1}{\alpha}))}{2\left(1 - \mu(1)(1 -\frac{1}{\alpha})\right)^2} \geq \frac{\mu(1)}{2}
\end{equation*}

\paragraph{Part 2 of the divergence:}
Since the input is drawn using product distribution
\begin{equation*}
    \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]}{\Pr[X_i = 1 | X_{-i}=0^{k-1}]}\right) = \Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]\log\left(\frac{\Pr[X_i = 1 | X_{-i}=0^{k-1}, \pi]}{\mu(1)}\right) = 
\end{equation*}
Using the previous lemma
\begin{equation*}
    \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}\log\left(\frac{\frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}}{\mu(1)}\right) = 
\end{equation*}
Some algebraic expansion
\begin{equation*}
    \frac{\mu(1)}{\lambda_i(\pi)\mu(0) + \mu(1)}\log\left(\frac{1}{\lambda_i(\pi)\mu(0) + \mu(1)}\right) = -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)}
\end{equation*}
Pay attention that since $ \mu(1) \geq 0$, $\mu(0) \geq \frac{1}{2}$ and $\lambda_i(\pi) \geq \alpha$
\begin{equation*}
    \lambda_i(\pi)\mu(0) + \mu(1) \geq \lambda_i(\pi)\mu(0) \geq \frac{\lambda_i(\pi)}{2} \geq \frac{\alpha}{2}
\end{equation*}
Since $\frac{log(x)}{x} \rightarrow 0$ when $x \rightarrow \infty$, for big enough $\alpha$
\begin{equation*}
    -\mu(1) \frac{\log(\lambda_i(\pi)\mu(0) + \mu(1))}{\lambda_i(\pi)\mu(0) + \mu(1)} \geq -\frac{\mu(1)}{4}
\end{equation*}
\paragraph{For conclusion}
If $\lambda_i(\pi) \geq \alpha$, for big enough $n$:
\begin{equation*}
     D_{KL}\left(\Dfrac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i} = 0^{k-1}} \right) \geq \frac{\mu(1)}{2} -\frac{\mu(1)}{4} = \frac{\mu(1)}{4}
\end{equation*}
\end{proof}

\paragraph{Part 3 - Information Summary}
\begin{theorem}
\begin{equation*}
    I(X;\Pi) \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\end{theorem}
\begin{proof}
By definition of $X$
\begin{equation*}
    I(X;\Pi) = I(X_i,X_{-i};\Pi)
\end{equation*}
Using chain rule for mutual information
\begin{equation*}
    I(X_i,X_{-i};\Pi) = I(X_{-i}; \Pi) + I(X_i;\Pi|X_{-i})
\end{equation*}
Since mutual information is nonnegative
\begin{equation*}
    \overbrace{I(X_{-i}; \Pi)}^{\geq 0} + I(X_i;\Pi|X_{-i}) \geq I(X_i;\Pi|X_{-i})
\end{equation*}
Using the mutual information - kl-divergence relation
\begin{equation*}
    I(X_i;\Pi|X_{-i}) = \mathbb{E}_{x, \pi \sim \mu} \left( D_{KL}\left(\Dfrac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Picking only a specific element ($x=0^k$) in the expectation sum
\begin{equation*}
    \mathbb{E}_{x, \pi \sim \mu} \left( D_{KL}\left(\Dfrac{X_i | \pi, X_{-i}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Looking only at transcripts in $A_{i}^\complement$
\begin{equation*}
    \mu(0^k) \mathbb{E}_{\pi \sim x = 0^k} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Using Part 1
\begin{equation*}
    \mu(0^k) \Pr[A_{i}^\complement| x = 0^k] \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right)
\end{equation*}
Using Part 2
\begin{equation*}
    \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \mathbb{E}_{\pi \sim x = 0^k| \pi \in A_{i}^\complement} \left( D\left(\frac{X_i | \pi, X_{-i} = 0^{k-1}}{X_i | X_{-i}} \right) \right) \geq \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4}
\end{equation*}
Since $\mu(1) = n^{-\frac{1}{k}}$ and $\mu(0) = 1 - n^{-\frac{1}{k}}$
\begin{equation*}
    \mu(0^k) \frac{1 - \epsilon (1 + \alpha ^k)}{k} \frac{\mu(1)}{4} = \left (1 - n^{-\frac{1}{k}} \right)^{k} \cdot \frac{1 - \epsilon (1 + \alpha ^k)}{k} \cdot \frac{n^{-\frac{1}{k}}}{4} \in \Omega\left(\frac{n^{-\frac{1}{k}}}{k}\right)
\end{equation*}
\end{proof}
\section{Extra}
\paragraph{Binary Distribution}
Let us consider a specific interesting distribution for $n = 2^{k-1}$. We think of $X_i \subseteq \{0, 1, ... , n-1\} $ \newline
For $i \in [k-1]$
  \[
    X_i=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| m_{i-1} = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| m_{i-1} = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
and for $i = k$
  \[
    X_k=\left\{
                \begin{array}{ll}
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 0\} \text{ w.p 0.5} \\
                  \{0 \leq m \leq n-1 \| \underset{i=1}{\overset{k-1}{\oplus}} m_i = 1\} \text{ w.p 0.5}
                \end{array}
              \right.
  \]
where $m_i$ is the $i$'th bit of m in binary representation. ($m_i \mathrel{\mathop:}= ( m \mathop{\&} 2^{i-1} ) \gg i-1 $). \\
Let us pay attention for some simple properties. First of all $\forall_i |X_i| = \frac{n}{2}$. Moreover, it does not matter if we permute the players, for $i < k, |A_i| = 0.5|A_{i-1}|$ and generally for $i < k, |A_i| = 2^{k-1-i}$. $\Pr[DISJ] = 0.5$. The thing is that this distribution has a little entropy ($k$).
\section{Appendix}


\subsection{TODO}
Add a universe set U. and pseudo code. 
Tough (est?) distribution\newline
Make sure to rethink how does this protocol end \newline
Does it help to permutate the players? \newline
Do we have to tell every player everything? maybe we can get rid of the k factor? I need to ask which one of them may have a critical index... \newline
\end{document}
